independent continuant;child-parents trio;a population of two parents and a child.
independent continuant;stratum population;a stratum population is a population resulting from a population stratification prior to sampling process which aims to produce homogenous subpopulations from an heterogeneous population by applying one or more stratification criteria
independent continuant;family;a domestic group, or a number of domestic groups linked through descent (demonstrated or stipulated) from a common ancestor, marriage, or adoption.
independent continuant;study group population;is a population whose individual members realize (may be expressed as) a combination of inclusion rule values specifications or resulting from a sampling process (e.g. recruitment followed by randomization to group) on which a number of measurements will be carried out, which may be used as input to statistical tests and statistical inference.
independent continuant;cohort;a cohort is a study group population where the members are human beings which meet inclusion criteria and undergo a longitudinal design
independent continuant;matched pair of subjects;a set of 2 subjects which result from a pairing process which assigns subject to a set based on a pairing rule/criteria
specifically dependent continuant;blocking variable;a blocking variable is a independent variable which is used in a blocking process part of an experiment with the purpose of maximizing the signal coming from the main variable.
specifically dependent continuant;experimental unit role;the role played by an entity part of study group as defined by an experimental design and realized in a data analysis and data interpretation
generically dependent continuant;absence of within subject difference hypothesis;is a null hypothesis stating that there are no difference observed across a series of measurements made one same subject.
generically dependent continuant;mortality;mortality is a ratio formed by the number of deaths due to a disease divided by the total population size.
generically dependent continuant;first order autoregressive moving average covariance structure;first order autoregressive moving average covariance structure is a type of covariance structure which is used in the context of time series analysis
generically dependent continuant;categorical variable;a categorical variable is a variable which that can only assume a finite number of value and cast observation in a small number of categories
generically dependent continuant;first quartile;the first quartile is a quartile which splits the lower 25 % of the data
generically dependent continuant;statistic estimator;a statistic estimator is a data item which is computed from a dataset to provide an approximated value (an estimator) for a 'statistical parameter' (a 'characteristics/parameter' of the true underlying distribution) of a real population.
generically dependent continuant;null hypothesis;a null hypothesis is a statistical hypothesis that is tested for possible rejection under the assumption that it is true (usually that observations are the result of chance). the concept was introduced by r. a. fisher. the hypothesis contrary to the null hypothesis, usually that the observations are the result of a real effect, is known as the alternative hypothesis.[wolfram alpha]
generically dependent continuant;absence of negative difference hypothesis;absence of negative difference hypothesis is a hypothesis which assumes that a difference significantly less than a threshold does not exist.
generically dependent continuant;tau squared;tau-squared is an estimate of the between-study variance in a random-effects meta-analysis. the square root of this number (i.e. tau) is the estimated standard deviation of underlying effects across studies.
generically dependent continuant;heterogeneous compound symmetry covariance structure;heterogenous compound symmetry structure is a compound symmetry covariance structure which has a different variance parameter for each diagonal element, and it uses the square roots of these parameters in the off-diagonal entries.
generically dependent continuant;count;a count is a data item denoted by an integer and represented the number of instances or occurences of an entity
generically dependent continuant;z-score;a z-score (also known as z-value, standard score, or normal score) is a measure of the divergence of an individual experimental result from the most probable result, the mean. z is expressed in terms of the number of standard deviations from the mean value.
generically dependent continuant;empirical measure;an empirical measure is a random measure arising from a particular realization of a (usually finite) sequence of random variables.
generically dependent continuant;covariance matrix;a covariance matrix is a square matrix that contains the variances and covariances associated with several variables. the diagonal elements of the matrix contain the variances of the variables and the off-diagonal elements contain the covariances between all possible pairs of variables.
generically dependent continuant;maf matrix;the maf matrix is a genomic relationship matrix which is obtained from the genotype matrix by counting the number of minor alleles at each locus
generically dependent continuant;spatial gaussian geometric anisotropic covariance structure;spatial gaussian geometric anisotropic covariance structure is a type of covariance structure characterized by its anisotropy, i.e., the variation of properties can be different in directions x and y, which is this case give gaussian features.
generically dependent continuant;cleveland dot plot;cleveland dot plot is a dot plot which plots points that each belong to one of several categories. they are an alternative to bar charts or pie charts, and look somewhat like a horizontal bar chart where the bars are replaced by a dots at the values associated with each category. compared to (vertical) bar charts and pie charts, cleveland argues that dot plots allow more accurate interpretation of the graph by readers by making the labels easier to read, reducing non-data ink (or graph clutter) and supporting table look-up.which
generically dependent continuant;chi-squared statistic;chi-squared statistic is a statistic computed from observations and used to produce a p-value in statistical test when compared to a chi-squared distribution.
generically dependent continuant;regression coefficient;a regression coefficient is a data item generated by a type of data transformation called a regression, which aims to model a response variable by expression the predictor variables as part of a function where variable terms are modified by a number. a regression coefficient is one such number.
generically dependent continuant;numerator degrees of freedom;the degree of freedom numerator is the number of degrees of freedom that the estimate of variance used in the numerator is based on. it is one of the parameters for the f-distribution used to compute probabilities in analysis of variance.
generically dependent continuant;false positive rate;a false positive rate is a data item which accounts for the proportion of incorrect rejection of a true null hypothesis.
generically dependent continuant;contrast;a contrast is the weighted sum of group means, the c j coefficients represent the assigned weights of the means (these must sum to 0 for orthogonal contrasts)
generically dependent continuant;weibull probability distribution;the weibull probability distribution is continuous probabibility distribution which is used to model time to fail, time to repair and material strength in material science. in biomedicine, the weibull probability is used to in determining 'hazard functions'. the 'location parameter' of the weibull probability distribution can be used to define a failure-free zone. if the quantity x is a "time-to-failure", the weibull distribution gives a distribution for which the failure rate is proportional to a power of time. the shape parameter, k, is that power plus one, and so this parameter can be interpreted directly as follows: a value of {\displaystyle k<1\,} {\displaystyle k<1\,} indicates that the failure rate decreases over time. this happens if there is significant "infant mortality", or defective items failing early and the failure rate decreasing over time as the defective items are weeded out of the population. in the context of the diffusion of innovations, this means negative word of mouth: the hazard function is a monotonically decreasing function of the proportion of adopters. a value of {\displaystyle k=1\,} {\displaystyle k=1\,} indicates that the failure rate is constant over time. this might suggest random external events are causing mortality, or failure. the weibull distribution reduces to an exponential distribution. a value of {\displaystyle k>1\,} {\displaystyle k>1\,} indicates that the failure rate increases with time. this happens if there is an "aging" process, or parts that are more likely to fail as time goes on. in the context of the diffusion of innovations, this means positive word of mouth: the hazard function is a monotonically increasing function of the proportion of adopters. the function is first concave, then convex with an inflexion point at {\displaystyle (e^{1/k}-1)/e^{1/k},k>1\,} {\displaystyle (e^{1/k}-1)/e^{1/k},k>1\,}.
generically dependent continuant;variable importance in projection;variable importance in projection is a measure computed as part of a partial least square regression to accumulate the importance of each variable j being reflected by w from each component.
generically dependent continuant;probability distribution;a probability distribution is a information content entity that specifies the probability of the value of a random variable. for a discrete random variable, a mathematical formula that gives the probability of each value of the variable. for a continuous random variable, a curve described by a mathematical formula which specifies, by way of areas under the curve, the probability that the variable falls within a particular interval.
generically dependent continuant;additive dominant genetic inheritance model;additive genetic model is a data item which refer to the contributions to the final phenotype from more than one gene, or from alleles of a single gene (in heterozygotes), that combine in such a way that the sum of their effects in unison is equal to the sum of their effects individually and dominance (of alleles at a single locus).
generically dependent continuant;compound symmetry covariance structure;compound symmetry covariance structure is a covariance structure which means that all the variances are equal and all the covariances are equal.
generically dependent continuant;mallows' cp;mallows' cp is a data item which compares the precision and bias of the full model to models with a subset of the predictors thus helping to choose between multiple regression models. the mallows cp is a function of the number of parameter used in the model relying on the residuals sum of squares to compute a score. the smaller cp is, the better the model fit is.
generically dependent continuant;spearman's rank correlation coefficient;spearman's rank correlation coefficient is a correlation coefficient which is a nonparametric measure of statistical dependence between two ranked variables. it assesses how well the relationship between two variables can be described using a monotonic function. if there are no repeated data values, a perfect spearman correlation of +1 or 1 occurs when each of the variables is a perfect monotone function of the other. spearman's coefficient may be used when the conditions for computing pearson's correlation are not met (e.g linearity, normality of the 2 continuous variables) but may require a ranking transformation of the variables
generically dependent continuant;skewness;skewness is a data item indicating of the degree of asymmetry of a distribution.
generically dependent continuant;mode;the mode is a data item which corresponds to the most frequently occurring number in a set of numbers.
generically dependent continuant;score;the score indicates how sensitive a likelihood function l(\theta,x) is to its parameter \theta. explicitly, the score for \theta is the gradient of the log-likelihood with respect to \theta.
generically dependent continuant;numerator relationship matrix;the numerator relationship matrix is the matrix of *expected* additive genetic relationships between individuals. this matrix was originally used by henderson (henderson, c.r. 1976. a simple method for computing the inverse of a numerator relationship matrix used in prediction of breeding values. biometrics 32:69-83.) to account for covariances between random effects, and therefore to use information from relatives in estimation of breeding value. among the properties of the nrm matrix (also known as the a matrix), it is symmetric, the diagonal value correspond to 1+ the inbreeding coefficient for an individual.
generically dependent continuant;fragments per kilobase of transcript per million fragments mapped;expected fragments per kilobase of transcript per million fragments mapped is a metric used to report transcript expression event as generated by rna-seq using paired-end library. the calculated value results from 2 types of normalization, one to take into account the difference in reads counts associated with transcript length (at equal abundance, longer transcripts will have more reads than shorter transcripts) , (hence the 'per kilobase of transcript') and the other one to take into account different sequencing depth during distinct sequencing runs (hence the 'per millions mapped fragment'. the metric is specifically produced by cufflink software.
generically dependent continuant;positive likelihood ratio;the likelihood ratio of positive results is a ratio which is form by dividing the sensitivity value of a test by the difference between 1 and specificity of the test. this can be expressed also as dividing the probability of the test giving a positive result when testing an affected subject versus the probability of the test giving a positive result when a subject is not affected.
generically dependent continuant;discrete multivariate probability distribution;a discrete multivariate probability distribution is a discrete probability distribution which describes the possible values, and corresponding probabilities, of two or more (usually three or more) associated random variables.
generically dependent continuant;false negative rate;the false negative rate is a data item which denotes the proportion of missed detection of elements known to be meeting the detection criteria
generically dependent continuant;wald statistic;the wald statistic is a statistic is used during a wald test, a test of significance of the regression coefficient. it is based on the asymptotic normality property of maximum likelihood estimates, and is computed as: w = b * 1/var(b) * b in this formula, b stands for the parameter estimates, and var(b) stands for the asymptotic variance of the parameter estimates. the wald statistic is tested against the chi-square distribution in the wald test.
generically dependent continuant;discrete probability distribution;a discrete probability distribution is a probability distribution which is defined by a probability mass function where the random variable can only assume a finite number of values or infinitely countable values
generically dependent continuant;pie chart;a pie chart is a graph in which a circular graph is divided into sector illustrating numerical proportion, meaning that the arc length of each sector (and consequently its central angle and area), is proportional to the quantity it represents.
generically dependent continuant;allele frequency;the allele frequency is a data item which denotes the incidence of a gene variant in a population. it is calculated as a ratio, by dividing the number of copies of a particular allele by the number of copies of all alleles at the genetic place (locus) in a population.
generically dependent continuant;measure of central tendency;a measure of central tendency is a data item which attempts to describe a set of data by identifying the value of its centre.
generically dependent continuant;real time quantitative pcr plot;a real time quantitative pcr plot is a line graph which plots the signal fluorescence intensity as a function of the number of pcr cycle
generically dependent continuant;pearson's correlation coefficient;the pearson's correlation coefficient is a correlation coefficient which evaluates two continuous variables for association strength in a data sample. it assumes that both variables are normally distributed and linearity exists. the coefficient is calculated by dividing their covariance with the product of their individual standard deviations. it is a normalized measurement of how the two are linearly related.
generically dependent continuant;design matrix;a design matrix is an information content entity which denotes a study design. the design matrix is a n by m matrix where n the number of rows, corresponds to the number of observations (4 rows if quadruplicates) and where m, the number of columns corresponds to the number of independent variables. each element in the matrix correspond to a discretized value representing one of the factor levels for a given factor. a design matrix can be used as input to statistical modeling or statistical analysis. the design matrix contains data on the independent variables (also called explanatory variables) in statistical models which attempt to explain observed data on a response variable (often called a dependent variable) in terms of the explanatory variables. the theory relating to such models makes substantial use of matrix manipulations involving the design matrix: see for example linear regression. a notable feature of the concept of a design matrix is that it is able to represent a number of different experimental designs and statistical models, e.g., anova, ancova, and linear regression
generically dependent continuant;number of orthogonal components;number of orthogonal components is a count used as input to the orthogonal partial least square discriminant analysis (opls-da)
generically dependent continuant;contrast estimate;estimate of a contrast obtained by computing the weighted sum of model parameter estimates using a set of contrast weights.
generically dependent continuant;one dimensional cartesian spatial coordinate origin;a cartesian spatial coordinate datum chosen as a fixed point of reference in a one dimensional spatial region.
generically dependent continuant;continuous probability distribution;a continuousprobability distribution is a probability distribution which is defined by a probability density function
generically dependent continuant;log signal intensity ratio;log signal intensity ratio is a data item which corresponding the logarithmitic base 2 of the ratio between 2 signal intensity, each corresponding to a condition.
generically dependent continuant;unstructured covariance structure;a covariance structure where no restrictions are made on the covariance between any pair of measurements.
generically dependent continuant;mcnemar test;mcnemar's test is a statistical test used on paired nominal data. it is applied to 2 2 contingency tables with a dichotomous trait, with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal (that is, whether there is "marginal homogeneity"). it is named after quinn mcnemar, who introduced it in 1947. an application of the test in genetics is the transmission disequilibrium test for detecting linkage disequilibrium
generically dependent continuant;volcano plot;volcano plot is a kind of scatter plot which graphs the negative log of the p-value (significance) on the y-axis versus log2 of fold-change between 2 conditions on the x-axis. it is a popular method for visualizing differential occurence of variables between 2 conditions.
generically dependent continuant;prevalence;prevalence is a ratio formed by the number of subjects diagnosed with a disease divided by the total population size.
generically dependent continuant;cartesian coordinate axis;a cartesian axis is one of 3 the axis in a cartesian coordinate system defining a referential in 3 dimensions. each of the axis is orthogonal to the other 2
generically dependent continuant;banded heterogeneous toeplitz covariance structure;the banded heterogeneous toeplitz covariance structure is a type of coviance structure which is often used to analyzed and intepret repeated measure design.
generically dependent continuant;2 by 2 contingency table;a 2x2 contingency table is a contingency table build for 2 dichotomous variables (i.e. 2 categorical variables, each with only 2 possible outcomes). it is the simplest of contingency tables
generically dependent continuant;channel1/channel2 fluorescence intensity ratio;is a data item formed by dividing the fluorescence intensity obtained in one channel to that obtained in the other channel, typically the case when considering 2-color microarray data when imaging is done for cy3 and cy5 dyes.
generically dependent continuant;measure of heterogeneity;a measure of heterogeneity in meta-analysis is a data item which aims to describe the variation in study outcomes between studies.
generically dependent continuant;cartesian coordinate system;a cartesian coordinate system is a coordinate system that specifies each point uniquely in a plane by a pair of numerical coordinates, which are the signed distances from the point to two fixed perpendicular directed lines, measured in the same unit of length.
generically dependent continuant;standard error of the mean;the standard error of the mean (sem) is data item denoting the standard deviation of the sample-mean's estimate of a population mean. it is calculated by dividing the sample standard deviation (i.e., the sample-based estimate of the standard deviation of the population) by the square root of n , the size (number of observations) of the sample.
generically dependent continuant;standard normal distribution;standard normal distribution is a normal distribution with variance = 1 and mean=0
generically dependent continuant;hypothesis;in statistics, a statement that can be tested.[wolfram alpha]
generically dependent continuant;mean difference;"the mean difference, or difference in means, measures the absolute difference between the mean value in two different groups."
generically dependent continuant;mixture distribution;mixture distribution is the probability distribution of a random variable that is derived from a collection of other random variables as follows: first, a random variable is selected by chance from the collection according to given probabilities of selection, and then the value of the selected random variable is realized.
generically dependent continuant;t-statistic;t-statistic is a statistic computed from observations and used to produce a p-value in statistical test when compared to a student's t distribution.
generically dependent continuant;trimmed mean;trimmed mean or truncated mean is a measure of central tendency which involves the calculation of the mean after discarding given parts of a probability distribution or sample at the high and low end, and typically discarding an equal amount of both
generically dependent continuant;minimum value;a minimum value is a data item which denotes the smallest value found in a dataset or resulting from a calculation.
generically dependent continuant;variance;variance is a data item about a random variable or probability distribution. it is equivalent to the square of the standard deviation. it is one of several descriptors of a probability distribution, describing how far the numbers lie from the mean (expected value).the variance is the second moment of a distribution.
generically dependent continuant;p matrix;p matrix is a kind of genomic relationship matrix which contains allele frequencies expressed as a difference from 0.5 and multiplied by 2.
generically dependent continuant;coefficient of determination;the coefficient of determination is a data item measuring the proportion of the variance in the dependent variable that is predictable from the independent variable(s). in the case of a linear regression mode, the coefficient of determination r2 is the quotient of the variances of the fitted values and observed values of the dependent variable.
generically dependent continuant;cylindrical coordinate system;a cylindrical coordinate system is a three-dimensional coordinate system that specifies point positions by the distance from a chosen reference axis, the direction from the axis relative to a chosen reference direction, and the distance from a chosen reference plane perpendicular to the axis. the latter distance is given as a positive or negative number depending on which side of the reference plane faces the point.
generically dependent continuant;model parameter estimate;a model parameter estimate is a data item which results from a model parameter estimation process and which provides a numerical value about a model parameter.
generically dependent continuant;polychoric correlation coefficient;polychoric correlation coefficient is a correlation coefficient which is computed over 2 variables to characterise an association by proxy with 2 (latent) variables which are assumed to be continuous and normally distributed.
generically dependent continuant;area under curve;area under curve is a measurement datum which corresponds to the surface define by the x-axis and bound by the line graph represented in a 2 dimensional plot resulting from an integration or integrative calculus. the interpretation of this measurement datum depends on the variables plotted in the graph
generically dependent continuant;sphericity hypothesis;a null hypothesis which states that a given matrix is proportional to a wishart-distributed covariance matrix
generically dependent continuant;ld plot;linkage disequilibrium plot is a graph which represents pairwise linkage disequilibrium measures between snp as a heatmap
generically dependent continuant;weighted arithmetic mean;the weighted arithmetic mean is a measure of central tendency that is the sum of the products of each observed value and their respective non-negative weights, divided by the sum of the weights, such that the contribution of each observed value to the mean may defer according to its respective weight. it is defined by the formula: a = sum(vi*wi)/sum(wi), where 'i' ranges from 1 to n, 'vi' is the value of each observation, and 'wi' is the value of the respective weight for each observed value. the weighted arithmetic mean is a kind of mean similar to an ordinary arithmetic mean (the most common type of average), except that instead of each of the data points contributing equally to the final average, some data points are weighted, meaning they contribute more than others. the weighted arithmetic mean is often used if one wants to combine average values from samples of the same population with different sample sizes.
generically dependent continuant;absence of between group difference hypothesis;it is a null hypothesis stating that there are no differences observed between group of subjects.
generically dependent continuant;whole plot number;a 'whole plot number' is a data item used to count and identify the actual piece of land (in the case of real field based trials) used in a split plot design experiment and receiving treatments corresponding to the levels of a factor whose randomization is restricted (these factors are known as 'hard to change' factors). in the case of non-field based trials, the 'whole plot' is a metaphor.
generically dependent continuant;range;the range is a measure of variation which describes the difference between the lowest score and the highest score in a set of numbers (a data set)
generically dependent continuant;credible interval;in bayesian statistics context, a credible interval is an interval of a posterior distribution which is such that the density at any point inside the interval is greater than the density at any point outside and that the area under the curve for that interval is equal to a prespecified probability level. for any probability level there is generally only one such interval, which is also often known as the highest posterior density region. unlike the usual confidence interval associated with frequentist inference, here the intervals specify the range within which parameters lie with a certain probability. the bayesian counterparts of the confidence interval used in frequentists statistics.
generically dependent continuant;lower confidence limit;lower confidence limit is a data item which is a lowest value bounding a confidence interval
generically dependent continuant;digital file;an electronic file is an information content entity which conforms to a specification or format and which is meant to hold data and information in digital form, accessible to software agents
generically dependent continuant;spherical coordinate system;in mathematics, a spherical coordinate system is a coordinate system for three-dimensional space where the position of a point is specified by three numbers: the radial distance of that point from a fixed origin, its polar angle measured from a fixed zenith direction, and the azimuth angle of its orthogonal projection on a reference plane that passes through the origin and is orthogonal to the zenith, measured from a fixed reference direction on that plane.
generically dependent continuant;outlier;outliers are deviant scores that have been legitimately gathered and are not due to equipment failures.
generically dependent continuant;ratio;a ratio is a data item which is formed with two numbers r and s is written r/s, where r is the numerator and s is the denominator. the ratio of r to s is equivalent to the quotient r/s. 
generically dependent continuant;reaction rate;reaction rate is a measurement datum which represents the speed of a chemical reaction turning reactive species into product species of event (i.e the number of such conversions)s occuring over a time interval
generically dependent continuant;number of cross-validation segments;number of cross-validation segments is a count which is used as input parameter in a cross validation procedure to evaluate a statistical model.
generically dependent continuant;chromosome coordinate system;chromosome coordinate system is a genomic coordinate which uses chromosome of a particular assembly build process to define start and end positions. this coordinate system is unstable and will change with each new genome sequence assembly build.
generically dependent continuant;equal variance testing objective;it is a testing objective to ensure the variances of the different groups used in a statistical test are similar (i.e. not too different).
generically dependent continuant;group comparison objective;group comparison objective is a data transformation objective which aims to determine if 2 or more study group differ with respect to the signal of a response variable
generically dependent continuant;funnel plot;a funnel plot is a scatter plot of treatment effect versus a measure of study size and aims to provide a visual aid to detecting bias or systematic heterogeneity. a symmetric inverted funnel shape arises from a well-behaved data set, in which publication bias is unlikely. an asymmetric funnel indicates a relationship between treatment effect and study size. known caveats: if high precision studies really are different from low precision studies with respect to effect size (e.g., due to different populations examined) a funnel plot may give a wrong impression of publication bias. the appearance of the funnel plot can change quite dramatically depending on the scale on the y-axis whether it is the inverse square error or the trial size. funnel plot was introduced by light and palmer in 1984.
generically dependent continuant;date;information about a calendar date or timestamp indicating day, month, year and time of an event.
generically dependent continuant;f-statistic;f statistic is a statistic computed from observations and used to produce a p-value in statistical test when compared to a f distribution. the f statistic is the ratio of two scaled sums of squares reflecting different sources of variability
generically dependent continuant;scree plot;a scree plot is a graphical display of the variance of each component in the dataset which is used to determine how many components should be retained in order to explain a high percentage of the variation in the data
generically dependent continuant;genomic estimated breeding value;genomic estimated breeding value (gebv) is an estimated breeding value derived from information in an organism dna (genotype). gebv is calculated differently to conventional estimated breeding values using advanced modeling technique to deal with high dimensionality data.
generically dependent continuant;eigenvalue;an eigenvalue is a data item resulting from a data transformation known as eigen value decomposition. it also corresponds to a process of matrix diagonalization or any equivalent operation, ie. transforming the underlying system of equations into a special set of coordinate axes in which the matrix takes this canonical form. each eigenvalue is paired with a corresponding so-called eigenvector.
generically dependent continuant;within subject comparison objective;the objective of a data transformation to test a null hypothesis of absence of difference within subject holds.
generically dependent continuant;full factorial design;a full factorial design is a factorial design which ensures that all possible factor level combinations are defined and used so all between group differences can be explored
generically dependent continuant;sequence read;a sequence read is a dna sequence data which is generated by a dna sequencer
generically dependent continuant;strictly standardized mean difference;strictly standardized mean difference (ssms) is a standardized mean difference which corresponds to the ratio of mean to the standard deviation of the difference between two groups. ssmd directly measures the magnitude of difference between two groups. ssmd is widely used in high content screen for hit selection and quality control. when the data is preprocessed using log-transformation as normally done in hts experiments, ssmd is the mean of log fold change divided by the standard deviation of log fold change with respect to a negative reference. in other words, ssmd is the average fold change (on the log scale) penalized by the variability of fold change (on the log scale). for quality control, one index for the quality of an hts assay is the magnitude of difference between a positive control and a negative reference in an assay plate. for hit selection, the size of effects of a compound (i.e., a small molecule or an sirna) is represented by the magnitude of difference between the compound and a negative reference. ssmd directly measures the magnitude of difference between two groups. therefore, ssmd can be used for both quality control and hit selection in hts experiments.
generically dependent continuant;sub-plot number;a 'sub plot number' is a data item used to count and identify the actual piece of land located within a 'whole plot', in the case of real field based trials using a split-plot design, and received completely randomized treatments corresponding to the factor levels combinations of the remainder factors declared in the experiment. in the case of 'split-split plot design', sub-plots also receive treatments corresponding to a factor whose randomization is restriction. in such configuration, each 'sub-plot' is itself divided into 'sub sub-plot', which then received the remainder of the treatments in completely randomized fashion. in the case of non-field based trials, the notion 'sub-plot' is a metaphor.
generically dependent continuant;standard deviation;the standard deviation of a random variable, statistical population, data set, or probability distribution is a measure of variation which correspond to the average distance from the mean of the data set to any given point of that dataset. it also corresponds to the square root of its variance.
generically dependent continuant;study group population size;statistical sample size is a count evaluating the number of individual experimental units
generically dependent continuant;absence of association hypothesis;a null hypothesis which states that no linkage exists between 2 categorical variables
generically dependent continuant;heterogeneous toeplitz covariance structure;this covariance structure has heterogenous variances and heterogenous correlations between elements. the correlation between adjacent elements is homogenous across pairs of adjacent elements. the correlation between elements separated by a third is again homogenous, and so on.
generically dependent continuant;effect size estimate;effect size estimate is a data item about the direction and strength of the consequences of a causative agent as explored by statistical methods. those methods produce estimates of the effect size, e.g. confidence interval
generically dependent continuant;number of degrees of freedom;the number degree of freedom is a count evaluating the number of values in a calculation that can vary. in statistics, the number of degrees of freedom is equal to n-1 in the case of the direct measurement of a quantity estimated by the arithmetic mean of n independent observations.
generically dependent continuant;covariance;the covariance is a measurement data item about the strength of correlation between a set (2 or more) of random variables. the covariance is obtained by forming: cov(x,y)=e([x-e(x)][y-e(y)] where e(x), e(y) is the expected value (mean) of variable x and y respectively. covariance is symmetric so cov(x,y)=cov(y,x). the covariance is usefull when looking at the variance of the sum of the 2 random variables since: var(x+y) = var(x) +var(y) +2cov(x,y) the covariance cov(x,y) is used to obtain the coefficient of correlation cor(x,y) by normalizing (dividing) cov(x,y) but the product of the standard deviations of x and y. 
generically dependent continuant;number of predictive components;number of predictive components is a count used as input to the principle component analysis (pca)
generically dependent continuant;rt-pcr standard curve;a real time pcr standard curve is a line graph which plots the fluorescence intensity signal as a function of the concentration of a sample used as reference and used to determine relative abundance of test samples
generically dependent continuant;sample mean;the sample mean of sample of size n with n observations is an arithmetic mean computed over n number of observations on a statistical sample. the sample mean, denoted x and read x-bar, is simply the average of the n data points x1, x2, ..., xn: x =x1+x2+ +xnn=1n i=1nxi the sample mean summarizes the "location" or "center" of the data. the sample mean is a measure of dispersion of the observations made on the sample and provides an unbias estimate of the population mean
generically dependent continuant;model interaction effect term;a model interaction effect term is a model term which accounts for variation explained by the combined effects of the factor levels of more than one (usually 2) independent variables.
generically dependent continuant;beta distribution;the beta distribution is a continuous probability distributions defined on the interval [0, 1] parametrized by two positive shape parameters, denoted by and , that appear as exponents of the random variable and control the shape of the distribution
generically dependent continuant;center value;the median is that value of the variate which divides the total frequency into two halves. the median is measure of central tendency of data. it is obtained by arranging the observations in order from smallest to largest value. if there is an odd number of observations, the median is the middle value. if there is an even number of observations, the median is the average of the two middle values.
generically dependent continuant;sequence read count;sequence read count is a data item determining how many sequence reads generated by a dna sequencing assay for a given stretch of dna can counted
generically dependent continuant;average log signal intensity;average log signal intensity is a data time which corresponds to the sum of 2 distinct logarithm base 2 transformed signal intensity, each corresponding to a distinct condition of signal acquisition, divided by 2.
generically dependent continuant;log likelihood;log likelihood is a data item which corresponds to the natural logarithm of the likelihood. log likelihood is a data item commonly used to provide a measure of accuracy of a model.
generically dependent continuant;statistic;a statistic is a measurement datum to describe a dataset or a variable. it is generated by a calculation on set of observed data.
generically dependent continuant;odds ratio homogeneity hypothesis;odds ratio homogeneity hypothesis is a null hypothesis stating that all odds ratio are homogenous, that is remain within the same range.
generically dependent continuant;model random effect term;the model random effect term is model term which aims to account for the unwanted variability in the data associated with a range of independent variables which are not the primary interest in the dataset. it is there also known as the variance component of the model
generically dependent continuant;spatial exponential geometric anisotropic covariance structure;spatial exponential geometric anisotropic covariance structure is a type of covariance structure characterized by its anisotropy, i.e., the variation of properties can be different in directions x and y, which is this case give exponential features.
generically dependent continuant;toeplitz covariance structure;the toeplitz covariance structure has homogenous variances and heterogenous correlations between elements. the correlation between adjacent elements is homogenous across pairs of adjacent elements. the correlation between elements separated by a third is again homogenous, and so on.
generically dependent continuant;outlier detection testing objective;outlier detection testing objective is a statistical objective of a data transformation which aims to test a null hypothesis that an observation is not an outlier.
generically dependent continuant;case-control study design;a case-control study design is a observation study design which assess the risk of particular outcome (a trait or a disease) associated with an event (either an exposure or endogenous factor). a case-control study design therefore declares an exposure variable which is dichotomous in nature (exposed/non-exposed) and an outcome variable, which is also dichotomous (case or control), thus giving the name to the design. during the execution of the design, a case control study defines a population and counts the events to determine their frequency.
generically dependent continuant;bar chart;the bart chart is a graph resulting from plotting rectangular bars with lengths proportional to the values that they represent.
generically dependent continuant;denominator degrees of freedom;the degree of freedom denominator is the number of degrees of freedom that the estimate of variance used in the denominator is based on. it is one of the parameters for the f-distribution used to compute probabilities in analysis of variance.
generically dependent continuant;z-statistic;z-statistic is a statistic computed from observations and used to produce a p-value when compared to a standard normal distribution in a statistical test called the z-test.
generically dependent continuant;y-axis;y-axis is a cartesian coordinate axis which is orthogonal to the x-axis and the z-axis
generically dependent continuant;scatterplot matrix;a scatterplot matrix contains all the pairwise scatter plots of a set of variables on a single page in a matrix format.
generically dependent continuant;alternative hypothesis;an alternative hypothesis is an hypothesis defined in a statistical test that is the opposite of the null hypothesis.
generically dependent continuant;absence of positive difference hypothesis;absence of negative difference hypothesis is a hypothesis which assumes that a difference significantly greater than a threshold does not exist.
generically dependent continuant;correlation coefficient;the correlation coefficient of two variables in a data sample is their covariance divided by the product of their individual standard deviations. it is a normalized measurement of how the two are linearly related.
generically dependent continuant;spatial exponential anisotropic covariance structure;spatial exponential anisotropic covariance structure is a type of covariance structure characterized by its anisotropy, i.e., the variation of properties can be different in directions x and y, which is this case give exponential features.
generically dependent continuant;residual;a residual is a data item which is the output of an error estimate or model fitting process and which is an observable estimate of the unobservable error
generically dependent continuant;model term;a model term is a data item set in statistical model formula to apportion source of variation.
generically dependent continuant;no diagonal factor analytic covariance structure;no diagonal factor-analytic covariance structure is a type of factor analytic covariance structure specified for q factors, which does not include a diagonal component for repeated measures.
generically dependent continuant;upper confidence limit;upper confidence limit is a data item which is a largest value bounding a confidence interval
generically dependent continuant;deviance;deviance is an indicator of fit and can be estimated by computing -2 times the log-likelihood ratio of the fitted model compared to a saturated(full) model. it is a generalization of the idea of using the sum of squares of residuals in ordinary least squares to cases where model-fitting is achieved by maximum likelihood.
generically dependent continuant;quartile;a quartile is a quantile which splits data into sections accrued of 25% of data, so the first quartile delineates 25% of the data, the second quartile delineates 50% of the data and the third quartile, 75 % of the data
generically dependent continuant;akaike information criterion;the akaike information criterion (aic) is a measure of the relative quality of a statistical model for a given set of data. as such, aic provides a means for model selection. aic is defined as: aic = 2k - 2log(l) where k is the number of predictors and l is the maximized likelihood value. aic deals with the trade-off between the goodness of fit of the model and the complexity of the model. it is founded on information theory: it offers a relative estimate of the information lost when a given model is used to represent the process that generates the data. aic does not provide a test of a model in the sense of testing a null hypothesis. i.e. aic can tell nothing about the quality of the model in an absolute sense. if all the candidate models fit poorly, aic will not give any warning of that. 
generically dependent continuant;forest plot;a forest plot is a graph designed to illustrate the relative strength of treatment effects in multiple quantitative scientific studies addressing the same question.
generically dependent continuant;normal distribution hypothesis;normal distribution hypothesis is a goodness of fit hypothesis stating that the distribution computed from the sample population fits a normal distribution.
generically dependent continuant;reads per kilobase of transcript per million fragments mapped;rpkm is a kind of count which numbers the sequence reads found per kilobase of transcript reported to million of sequence reads. rpkm is a metric generated by erange software tool as reported by mortazi et al, in 2008. the metric has been enhanced and replaced by fpkm to better take into account splice variant. fkpm uses a statistical model to perform the computation.
generically dependent continuant;z-axis;z-axis is a cartesian coordinate axis which is orthogonal to the x-axis and the y-axis
generically dependent continuant;axis;an axis is a line graph used as reference line for the measurement of coordinates.
generically dependent continuant;genotype matrix;a genotype matrix is a kind of genomic relationship matrix in the rawest of form and which simply corresponds to a matrix of individuals genotype for a given set of markers or genomic positions. columns are snps or markers, rows are individuals. each column/row cell contains a genotype expressed as, in the genome is diploid, as a pair of characters chosen from atgc where the dominant variant is uppercased and the recessive variant is lower cased.
generically dependent continuant;ordinal variable;ordinal variable is a categorical variable where the discrete possible values are ordered or correspond to an implicit ranking
generically dependent continuant;hotelling t2 distribution;hotelling t squared distribution is a probability distribution used in multivariate hypothesis testing, which is a univariate distribution proportional to the f-distribution and arises importantly as the distribution of a set of statistics which are natural generalizations of the statistics underlying student's t-distribution. in particular, the distribution arises in multivariate statistics in undertaking tests of the differences between the (multivariate) means of different populations, where tests for univariate problems would make use of a t-test. the distribution is named for harold hotelling, who developed it[1] as a generalization of student's t-distribution. this distribution is commonly used to describe the sample mahalanobis distance between two populations.
generically dependent continuant;confidence level;the frequency (i.e., the proportion) of possible confidence intervals that contain the true value of their corresponding parameter. in other words, if confidence intervals are constructed using a given confidence level in an infinite number of independent experiments, the proportion of those intervals that contain the true value of the parameter will match the confidence level. a probability measure of the reliability of an inferential statistical test that has been applied to sample data and which is provided along with the confidence interval for the output statistic.
generically dependent continuant;absence of difference hypothesis;a null hypothesis which states that no difference exists between 2 or more groups being considered.
generically dependent continuant;wilkinson dot plot;the dot plot as a representation of a distribution consists of group of data points plotted on a simple scale. dot plots are used for continuous, quantitative, univariate data. data points may be labelled if there are few of them.
generically dependent continuant;hedges's g;hedges's g is an estimator of effect size, which is similar to cohen's d and is a measure based on a standardized difference. however, the denominator, corresponding to a pooled standard deviation, is computed differently from cohen's d coefficient, by applying a correction factor (which involves a gamma function).
generically dependent continuant;probability distribution location parameter;a probability distribution location parameter is a data item which is set by the operator when selecting a parametric probability distribution and which dictates the way the location but not the profile or size of the distribution plot looks like.
generically dependent continuant;pareto type-i probability distribution;the pareto distribution is a continuous probability distribution, which is defined by the follwoing probability density (1) function and distribution function (2) (1): p(x)=(ab^a)/(x^(a+1)) (2): d(x)=1-(b/x)^a defined over the interval x>=b.
generically dependent continuant;model error term;a model error term is a model term which accounts for residual variation not explained by the other components (fixed and random effect terms)
generically dependent continuant;violin plot;a violin plot is a plot combining the features of box plot and kernel density plot. the violin plot is therefore similar to box plot but it incorporated in the display the probability density of the data at different values. typically violin plots will include a marker for the median of the data and a box indicating the interquartile range, as in standard box plots.
generically dependent continuant;survival distribution;probability distribution estimated empirically from a censored lifetime data
generically dependent continuant;sphericity testing objective;sphericity testing objective is a statistical objective of a data transformation which aims to test a null hypothesis of sphericity holds.
generically dependent continuant;association between categorical variables testing objective;the objective of a data transformation to evaluate a null hypothesis of absence of linkage between variables.
generically dependent continuant;contingency table;a contigency table is a data item which displays the (multivariate) frequency distribution of the possible values of categorical variables. the first row of the table corresponds to categories of one categorical variable, the first column of the table corresponds to categories of the other categorical variable, the cells corresponding to each combination of categories is filled with the observed occurences in the sample being considered. the table also contains marginal total (marginal sums) and grand total of the occurences the term contingency table was first used by karl pearson in "on the theory of contingency and its relation to association and normal correlation", part of the drapers' company research memoirs biometric series i published in 1904.
generically dependent continuant;presence of between group difference hypothesis;a null hypothesis stating that there are differences observed between group of subjects
generically dependent continuant;95% confidence interval;a confidence interval which covers 95% of the sampling distribution, meaning that there is a 5% risk of false positive (type i error). if the number of observations made is large enough, the sampling distribution can be assumed to be normal, which entails that 95% of the sampling distributions falls within roughly2 (1.96) standard deviations from the mean.
generically dependent continuant;focused information criterion;the focused information criterion is a measurement data item which aims at facilitating model selection. it was published in 2003 by claeskens, g. and hjort, n.l. (2003). "the focused information criterion".
generically dependent continuant;ante-dependence covariance structure;ante-dependence covariance structure is a covariance structure which specifies that the covariance between two time points is a function of the product of variances at both points (hence allowing hetrogenity of error variance across measures to affect the correlation) and the product of the correlations at the distances up to the one chosen.
generically dependent continuant;box and whisker plot;a box plot is a graph which plots datasets relying on their quartiles and the interquartile range to create the box and the whiskers.
generically dependent continuant;equal diagonal factor analytic covariance structure;factor-analytic structure is a covariance structure which is specified for q factors equal diagonal factor-analytic covariance structure is a type of factor analytic covariance structure specified for q factors, which includes a diagonal component for repeated measures.
generically dependent continuant;i-squared;the quantity called i2, describes the percentage of total variation across studies that is due to heterogeneity rather than chance. i2 can be readily calculated from basic results obtained from a typical meta-analysis as i2 = 100% (q - df)/q, where q is cochran's heterogeneity statistic and df the degrees of freedom. negative values of i2 are put equal to zero so that i2 lies between 0% and 100%. a value of 0% indicates no observed heterogeneity, and larger values show increasing heterogeneity. unlike cochran's q, it does not inherently depend upon the number of studies considered. a confidence interval for i is constructed using either i) the iterative non-central chi-squared distribution method of hedges and piggott (2001). or ii) the test-based method of higgins and thompson (2002). the non-central chi-square method is currently the method of choice (higgins, personal communication, 2006) it is computed if the 'exact' option is selected.
generically dependent continuant;non-parametric distribution;probability distribution estimated empirically on the data without assumptions on the shape of the probability distribution.
generically dependent continuant;observed risk;the proportion of individuals in a population with the outcome of interest
generically dependent continuant;between group comparison objective;the objective of a data transformation to test a null hypothesis of absence of difference withing subject holds.
generically dependent continuant;continuous variable;a continuous variable is one for which, within the limits the variable ranges, any value is possible.
generically dependent continuant;empirical distribution;probability distribution estimated empirically from all acquired data
generically dependent continuant;three dimensional cartesian coordinate system;a three dimensional cartesian coordinate system is a cartesian coordinate system which defines 3 orthogonal one dimensional axes and which may be used to describe a 3 dimensional spatial region.
generically dependent continuant;laplace probability distribution;the double exponential distribution (a.k.a. laplace distribution) is the distribution of differences between two independent variates with identical exponential distributions (abramowitz and stegun 1972, p. 930).
generically dependent continuant; rn;(rn +) (rn ), where rn + = (emission intensity of reporter dye)/(emission intensity of passive reference dye) in pcr with template and rn = (emission intensity of reporter dye)/(emission intensity of passive reference dye) in pcr without template or early cycles of a real-time reaction. ct = threshold cycle, i.e., cycle at which a statistically significant increase in rn is first detected
generically dependent continuant;factor level;a factor level is data item which corresponds to one of the value assumed by a factor or independent variable manipulated and set by the experimentalist. in the context of factorial design, a factor level is assumed to be or treated as a category in a categorical variable
generically dependent continuant;quadratic mean;the root mean square (abbreviated rms or rms), also known as the quadratic mean, in statistics is a statistical measure of central tendency defined as the square root of the mean of the squares of a sample. ( to find the root mean square of a set of numbers, square all the numbers in the set and then find the arithmetic mean of the squares. take the square root of the result. this is the root mean square.)
generically dependent continuant;pairing rule;pairing rule is a rule which is specifies the criteria for deciding on how to associated any 2 entities.
generically dependent continuant;contrast weight estimate;contrast weight estimate is a model parameter estimate which results from the computation from the data and that is used as input to a model fitting process
generically dependent continuant;kernel mixture distribution;kernel density estimation (kde) is a non-parametric way to estimate the probability density function of a random variable
generically dependent continuant;chi-square probability distribution;chi-square probability distribution with k degrees of freedom is a theoretical probability distribution which corresponds to the distribution of a sum of the squares of k independent standard normal random variables.
generically dependent continuant;glass's delta;glass's delta is an estimator of effect size which is similar to cohen's d but where the denominator corresponds only to the standard deviation of the control group (or second group). it is considered less biais than the cohen's d for estimating effect sizes based on means and distances between means.
generically dependent continuant;bayesian model;a bayesian model is a statistical model where inference is based on using bayes theorem to obtain a posterior distribution for a quantity (or quantities) of interest for some model (such as parameter values) based on some prior distribution for the relevant unknown parameters and the likelihood from the model.
generically dependent continuant;state space model;a state space model is a kind of statistical model which describes the probabilistic dependence between the latent state variable and the observed measurement. the state or the measurement can be either continuous or discrete. the term state space originated in 1960s in the area of control engineering (kalman, 1960). ssm provides a general framework for analyzing deterministic and stochastic dynamical systems that are measured or observed through a stochastic process.
generically dependent continuant;balanced design;a balanced design is a an experimental design where all experimental group have the an equal number of subject observations
generically dependent continuant;pareto type-iv probability distribution;the pareto(iv) distribution is a continous probability distribution which is described with a cumulative distribution function of the following form: f(y) = 1 [1 + ((y a)/b)1/g] s for y > a, b > 0, g > 0 and s > 0. a is the location parameter, b is the scale parameter, g is the inequality parameter s is the shape parameter the distribution is used in actuarial science, economics, finance and telecommunications, but not restricted to those fields.
generically dependent continuant;threshold cycle;threshold cycle (or ct or cq) is a count which is defined as the fractional pcr cycle number at which the reporter fluorescence is greater than the threshold in the context of the rt-qpcr assay. the ct is a basic principle of real time pcr and is an essential component in producing accurate and reproducible data.
generically dependent continuant;polar coordinate system;in mathematics, the polar coordinate system is a two-dimensional coordinate system in which each point on a plane is determined by a distance from a fixed point and an angle from a fixed direction.
generically dependent continuant;genotype data set;a dataset which is made up of genotypic information, that is presenting allele information at specific loci in a set of individuals of an organism.
generically dependent continuant;interquartile mean;the interquartile mean (iqm) (or midmean) is a statistical measure of central tendency based on the truncated mean of the interquartile range. in the calculation of the iqm, only the data in the second and third quartiles is used (as in the interquartile range), and the lowest 25% and the highest 25% of the scores are discarded. these points are called the first and third quartiles, hence the name of the iqm.
generically dependent continuant;fixed effect model;a fixed effect model is a statistical model which represents the observed quantities in terms of explanatory variables that are treated as if the quantities were non-random.
generically dependent continuant;student's t distribution;the student's t distribution is a continuous probability distribution which arises when estimating the mean of a normally distributed population in situations where the sample size is small and population standard deviation is unknown.
generically dependent continuant;geometric mean;the geometric mean is defined as the nth root of the product of n numbers, i.e., for a set of numbers \{x i\} {i=1}^n, the geometric mean is defined as \left(\prod {i=1}^n x i\right)^{1/n}.
generically dependent continuant;split split plot design;a split split plot design is a study design where restricted randomization affect 2 study factors (and not 1 as in split-plot design). such design is only possible if at least 3 independent variables are present. 
generically dependent continuant;data distribution;variable distribution is data item which denotes the spatial resolution of data point making up a variable. variable distribution may be compared to a known probability distribution using goodness of fit test or plotting a quantile-quantile plot for visual assessment of the fit.
generically dependent continuant;genomic coordinate system;a genomic coordinate system is a coordinate system to describe position of sequence on a genomic scaffold (assembly of chromosome, contig....)
generically dependent continuant;z matrix;the z-matrix is a genomic relationship matrix which is obtained by substracted the m matrix with the p matrix. it is also known as the incidence matrix for the markers.
generically dependent continuant;true positive rate;sensitivity is a measurement datum qualifying a binary classification test and is computed by substracting the false negative rate to the integral numeral 1
generically dependent continuant;block design;a block design is a kind of study design which declares a blocking variable (also known as nuisance variable) in order to account for a known source of variation and reduce its impact on the acquisition of the signal
generically dependent continuant;f-distribution;the f-distribution is a continuous probability distribution which arises in the testing of whether two observed samples have the same variance.
generically dependent continuant;smooth kernel distribution;probability distribution estimated using a smooth kernel function to avoid making assumptions about the distribution of the data. the kernel density estimator is the estimated probability density function (pdf) of the random variable.
generically dependent continuant;heritability;in a planned experiment where to covariance (genotype x environment) can be controlled and held at 0, the heritability is defined as the ratio of the variance of the genotypic variables to the variance of the phenotypic variables. h2 = var(g)/var(p) h2 is the broad-sense heritability. this reflects all the genetic contributions to a population's phenotypic variance including additive, dominant, and epistatic (multi-genic interactions), as well as maternal and paternal effects, where individuals are directly affected by their parents' phenotype, for example, milk production in mammals.
generically dependent continuant;dersimonian-laird estimator;dersimonian-laird estimator s a data item computed to estimate heterogeneity parameter (estimate of between-study variance) in a random effect model for meta analysis. the estimator is used in simple noniterative procedure for characterizing the distribution of treatment effects in a series of studies
generically dependent continuant;homoskedasticity hypothesis;homoskedasticity states that all variances under consideration are homogenous.
generically dependent continuant;risk difference;the risk difference is the difference between the observed risks (proportions of individuals with the outcome of interest) in the two groups. the risk difference is straightforward to interpret: it describes the actual difference in the observed risk of events between experimental and control interventions.
generically dependent continuant;histogram distribution;probability distribution estimated empirically on the data following a binning process
generically dependent continuant;covariance structure;a covariance structure is a data item which is part of a regression model and which indicates a pattern in the covariance matrix. the nature of covariance structure is specified before the regression analysis and various covariance structure may be tested and evaluated using information criteria to help choose the most suiteable model
generically dependent continuant;number of pcr cycle;number of pcr cycle is a count which enumerates how many iterations of 'annealing, renaturation, amplification,' rounds (or cycles) are performed during a polymerase chain reaction (pcr) or an assay relying on pcr.
generically dependent continuant;random effect model;a random effect(s) model, also called a variance components model, is a kind of hierarchical linear model. it assumes that the dataset being analysed consists of a hierarchy of different populations whose differences relate to that hierarchy.
generically dependent continuant;exponential distribution;the exponential distribution (a.k.a. negative exponential distribution) is the probability distribution that describes the time between events in a poisson process, i.e. a process in which events occur continuously and independently at a constant average rate. it is the continuous analogue of the geometric distribution, and it has the key property of being memoryless.
generically dependent continuant;goodness of fit testing objective;a testing objective to ensure that the sample used in a statistical test actually follows a normal distribution.
generically dependent continuant;fold change;fold change is a number describing how much a quantity changes going from an initial to a final value or one condition to another condition
generically dependent continuant;residual mean square;a residual mean square is a data item which is obtained by dividing the sum of squared residuals (ssr) by the number of degrees of freedom
generically dependent continuant;scale estimator;a scale estimator is a measurement datum (a statistic) which is calculated to approach the actual scale parameter of a probability distribution from observed data.
generically dependent continuant;99% credible interval;in bayesian statistics context, a 99% credible interval is a credible interval which, given the data, includes the true parameter with probability of 99%.
generically dependent continuant;multinomial distribution;the multinomial distribution is a probability distribution which gives the probability of any particular combination of numbers of successes for various categories defined in the context of n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability.
generically dependent continuant;substrate concentration;substrate concentration is a scalar measurement datum which denotes the amount of molecular entity involved in an enzymatic reaction (or catalytic chemical reaction) and whose role in that reaction is as substrate.
generically dependent continuant;90% confidence interval;a confidence interval which covers 90% of the sampling distribution, meaning that there is a 90% risk of false positive (type i error)
generically dependent continuant;three dimensional cartesian spatial coordinate origin;a cartesian spatial coordinate datum chosen as a fixed point of reference in a three dimensional spatial region.
generically dependent continuant;grouped bar chart;grouped bar chart is a kind of bar chart which juxtaposes the discrete values for each of the possible value of a given categorical variable, thus providing within group comparison. grouped bar charts are good for comparing between each element in the categories, and comparing elements across categories. however, the grouping can make it harder to tell the difference between the total of each group.
generically dependent continuant;split-plot design;a split-plot design is kind of factorial design which is used when running a full factorial completely randomized design is inpractical, either for cost or practicalities (e.g. equipment, fields), in other words, when a restricted randomization has to be applied. a split-plot design is used whenever practioners fix the level of 'hard to change factor' and run all the combinations of the other factors. the hard to change factor is also refered to as the 'whole plot' factor, while the remainders of the factors are refered to as 'split plot factor'. performing a split-plot design therefore means fixing one factor level, and then applying the treatments formed by the cartesian products of the levels for the other factors. a mininum of 2 factors are required and one being applied before the other(s).
generically dependent continuant;posterior probability distribution;a posterior probability distribution is a probability distribution computed in a bayesian model approach given a prior distribution and a set of events/observations.
generically dependent continuant;pretest probability;the probability of a patient having the target disorder before a diagnostic test result is known
generically dependent continuant;factor analytic covariance structure;factor-analytic structure is a type of heterogeneous covariance structure which is specified for q factors
generically dependent continuant;m matrix;the m matrix is a genomic relationship matrix which is obtained by subtracting 1 to every value of the maf matrix (gene content matrix). the values of the m matrix are only -1, 0 or 1 and makes computation easier. m = maf-1
generically dependent continuant;harmonic mean;the harmonic mean is a kind of mean which is calculated by dividing the total number of observations by the reciprocal of each number in a series. harmonic mean = n/(1/a1+1/a2+1/a3+1/a4+.......+1/an) where a(i)= individual score and n = sample size (number of scores)
generically dependent continuant;expected value;the expected value (or expectation, mathematical expectation, ev, mean, or the first moment) of a random variable is a data item which corresponds to the weighted average of all possible values that this random variable can take on. the weights used in computing this average correspond to the probabilities in case of a discrete random variable, or densities in case of a continuous random variable. from a rigorous theoretical standpoint, the expected value is the integral of the random variable with respect to its probability measure.
generically dependent continuant;polychotomous variable;a polychotomous variable is a categorical variable which is defined to have minimally 2 categories or possible values
generically dependent continuant;mixed effect model;a mixed model is a statistical model containing both fixed effects and random effects. these models are useful in a wide variety of disciplines in the physical, biological and social sciences. they are particularly useful in settings where repeated measurements are made on the same statistical units (longitudinal study), or where measurements are made on clusters of related statistical units. because of their advantage in dealing with missing values, mixed effects models are often preferred over more traditional approaches such as repeated measures anova.
generically dependent continuant;continuous multivariate probability distribution;a continuous multivariate probability distribution is a continuous probability distribution which describes the possible values, and corresponding probabilities, of two or more (usually three or more) associated random variables.
generically dependent continuant;eta-squared;eta-squared is a biased estimator of the variance explained by the model in the population (it estimates only the effect size in the sample). eta-squared describes the ratio of variance explained in the dependent variable by a predictor while controlling for other predictors, making it analogous to the r2. this estimate shares the weakness with r2 that each additional variable will automatically increase the value of 2. in addition, it measures the variance explained of the sample, not the population, meaning that it will always overestimate the effect size, although the bias grows smaller as the sample grows larger.
generically dependent continuant;standardized mean difference;standardized mean difference is data item computed by forming the difference between two means, divided by an estimate of the within-group standard deviation. it is used to provide an estimatation of the effect size between two treatments when the predictor (independent variable) is categorical and the response(dependent) variable is continuous
generically dependent continuant;narrow sense heritability;a particularly important component of the genetic variance is the additive variance, var(a), which is the variance due to the average effects (additive effects) of the alleles. since each parent passes a single allele per locus to each offspring, parent-offspring resemblance depends upon the average effect of single alleles. additive variance represents, therefore, the genetic component of variance responsible for parent-offspring resemblance. the additive genetic portion of the phenotypic variance is known as narrow-sense heritability and is defined as: h2 = var(a)/var(p)
generically dependent continuant;completely randomized design;a completely randomized design is a type of design of experiment where the observation unit receive treatments (independent variable level) entirely at random. in other words, the observations unit are randomly assigned to treatments. completely randomized designs differ from randomized complete block design and should not be confused as in the latter, a blocking variable is first use to assign experimental units to blocks. then only, the members of each block are then randomly assigned to different treatment groups
generically dependent continuant;deviance information criterion;the deviance information criterion (dic) is a hierarchical modeling generalization of the aic (akaike information criterion) and bic (bayesian information criterion, also known as the schwarz criterion). it is particularly useful in bayesian model selection problems where the posterior distributions of the models have been obtained by markov chain monte carlo (mcmc) simulation. like aic and bic it is an asymptotic approximation as the sample size becomes large. it is only valid when the posterior distribution is approximately multivariate normal. the deviance information criterion was published in 2002 by spiegelhalter et al. spiegelhalter, d. j., n. g. best, b. p. carlin, and a. van der linde, 2002. bayesian measures of model complexity and fit. journal of the royal statistical society, b, 64, 583-639.
generically dependent continuant;dimensionless ratio;a ratio where the numerator and denominator are expressed in the same unit.
generically dependent continuant;selectivity ratio;the selectivity ratio (sr) is defined as the ratio of explained vexpl,i to residual variance vres,i for the variable i on the target projection (tp) component in the context of partial least squares analysis.
generically dependent continuant;manhattan plot for gwas;a manhattan plot for gwas is a kind of scatter plot used to facilitate presentation of genome-wide association study (gwas) data. genomic coordinates are displayed along the x-axis, with the negative logarithm of the association p-value for each single nucleotide polymorphism displayed on the y-axis.
generically dependent continuant;power law distribution;a power-law probability distribution is a probability distribution whose density function (or mass function in the discrete case) has the form p(x) = l(x) . x^{-alpha} where alpha is a parameter >1 and l(x) is a slowly varying function. 
generically dependent continuant;third quartile;the first quartile is a quartile which splits the 75 % of the data
generically dependent continuant;2 by n contingency table;a 2 by n contingency table is a contingency table built for one dichotomous variable (a categorical variable with only 2 outcomes) and one polychotomous variable (a polychomotomous variable with at least 2 outcomes)
generically dependent continuant;inter quartile range;the interquartile range is a data item which corresponds to the difference between the upper quartile (3rd quartile) and lower quartile (1st quartile). the interquartile range contains the second quartile or median. the interquartile range is a data item providing a measure of data dispersion 
generically dependent continuant;stacked bar chart;stacked bar chart is a bar which is used to compare overall quantities across items while showing the contribution of category to the total amount. stacked bar chart can be used for highlighting the total as they visually aggregate all of the categories in a group while indicating a part to whole relationship. the downside is that it becomes harder to compare the sizes of the individual categories.
generically dependent continuant;scaled t distribution;a scaled t distribution is a kind of student's t distribution which is shifted by 'mean' and scaled by standard deviation 'sd'.
generically dependent continuant;two dimensional cartesian spatial coordinate origin;a cartesian spatial coordinate datum chosen as a fixed point of reference in a two dimensional spatial region.
generically dependent continuant;gamma distribution;a gamma distribution is a general type of continous statistical distribution (related to the beta distribution) that arises naturally in processes for which the waiting times between poisson distributed events are relevant. gamma distributions have two free parameters shape k and scale denoted theta .
generically dependent continuant;standard error of a contrast estimate;an estimate of the standard deviation of a contrast estimate sampling distribution.
generically dependent continuant;sub sub-plot number;a 'sub sub-plot number' is a data item used to count and identify the actual piece of land located within a 'sub plot', in the case of real field based trials using a split-split-plot design, and received completely randomized treatments corresponding to the factor levels combinations of the remainder factors declared in the experiment. in the case of 'split-split plot design', sub-plots also receive treatments corresponding to a factor whose randomization is restriction. in such configuration, each 'sub-plot' is itself divided into 'sub sub-plot', which then received the remainder of the treatments in completely randomized fashion. in the case of non-field based trials, the notion 'sub sub-plot' is a metaphor.
generically dependent continuant;geometric distribution;the geometric distribution is a negative binomial distribution where r is 1. it is useful for modeling the runs of consecutive successes (or failures) in repeated independent trials of a system. the geometric distribution models the number of successes before one failure in an independent succession of tests where each test results in success or failure. the geometric distribution with prob = p has density p(x) = p (1-p)^x for x = 0, 1, 2, , 0 < p 1. if an element of x is not integer, the result of dgeom is zero, with a warning. the quantile is defined as the smallest value x such that f(x) p, where f is the distribution function.
generically dependent continuant;odds ratio;odds ratio is a ratio that measures effect size, that is the strength of association between 2 dichotomous variables, one describing an exposure and one describing an outcome. it represents the odds that an outcome will occur given a particular exposure, compared to the odds of the outcome occurring in the absence of that exposure ( the probability of the event occuring divided by the probability of an event not occurring). the odds ratio is a ratio of describing the strength of association or non-independence between two binary data values by forming the ratio of the odds for the first group and the odds for the second group. odds ratio are used when one wants to compare the odds of something occurring to two different groups.
generically dependent continuant;number of factor level combinations;a non-negative integer defining how many combination of factor levels (or treatments in the statistical sense) are to be used in a study.
generically dependent continuant;central composite design;a central composite design is a study design which contains an imbedded factorial or fractional factorial design with center points that is augmented with a group of so-called 'star points' that allow estimation of curvature. a ccd design with k factors has 2k star points.
generically dependent continuant;genomic coordinate datum;genomic coordinate datum is a data item which denotes a genomic position expressed using a genomic coordinate system
generically dependent continuant;cochran's q statistic;the cochran's q statistic is a measure of heterogeneity accros study computed by summing the squared deviations of each study's estimate from the overall meta-analytic estimate, weighting each study's contribution in the same manner as in the meta-analysis.
generically dependent continuant;quantile;a quantile is a data item which corresponds to specific elements x in the range of a variate x. the k-th n-tile p k is that value of x, say x k, which corresponds to a cumulative frequency of nk/n (kenney and keeping 1962). if n=4, the quantity is called a quartile, and if n=100, it is called a percentile.
generically dependent continuant;contrast weight matrix;a contrast weight matrix is a information content entity which holds a set of contrast weight, coefficient used in a weighting sum of means defining a contrast
generically dependent continuant;pedigree chart;a pedigree chart is a graph which plots parent child relations
generically dependent continuant;polynomial contrast;a polynomial contrast is a contrast which...
generically dependent continuant;graeco-latin square design;graeco-latin square design is a study design which allows in its simpler form controlling 3 levels of nuisance variables (also known as blocking variables). the 3 nuisance factors are divided into a tabular grid with the property that each row and each column receive each treatment exactly once.
generically dependent continuant;estimated breeding value;the estimated breeding value of an organism is a data item computed to estimate the true breeding value defined as genetic merit of an organism, half of which will be passed on to its progeny. while the exact breeding value can not been known, for performance traits it is possible to make good estimates. these estimates are called estimated breeding values (ebvs). ebvs are expressed in the units of measurement for each particular trait. these estimates are output of various estimation methods which differ depending on the underlying assumptions (equal variance of marker effect, all markers contributing to the trait) , the mathemical methods used (bayesian or non-bayesians) and the genetic inheritance models being considered (additive, dominant, epistatic) selected by the analysts.
generically dependent continuant;variable;a variable is a data item which can assume any of a set of values, either as determined by an agent or as randomly occuring through observation.
generically dependent continuant;augmented experimental design;augmented design is a kind of experimental design where the goal is to compare existing (control) treatments with new treatments that have an experimental constraint of "limited replication". to understand limited replication, consider about experiments that may only allow a single representation of the new treatment, this limitation may be many times due to the cost associated with the experiment, limited resources, or limited number of new units that can be used in the experiment. in contrast, the existing treatments are referred as checks and are generally replicated multiple times. with augmented design one can estimate the following: a) differences between checks and new treatments, b) differences among new treatments, c) differences among check treatments, and d) differences among new and check treatments combined.
generically dependent continuant;lineweaver-burk plot;lineweaver-burk plot is a graph which is the graphical representation of the lineweaver burk equation of enzyme kinetics, described by hans lineweaver and dean burk in 1934. the plot provides a useful graphical method for analysis of the michaelis menten equation. it was widely used to determine important terms in enzymology and enzyme kinetics as the x-intercept of the graph represents 1/km and the y-intercept of such a graph is equivalent to the inverse of vmax
generically dependent continuant;hunter-schmidt estimator;hunter-schmidt estimator is a data item computed to estimate heterogeneity parameter (estimate of between-study variance) in a random effect model for meta analysis.
generically dependent continuant;additive dominant genetic and epistatic inheritance model;additive genetic model ris a data item which refer to the contributions to the final phenotype from more than one gene, or from alleles of a single gene (in heterozygotes), that combine in such a way that the sum of their effects in unison is equal to the sum of their effects individually and additive dominant ( (of alleles at a single locus) ) and epistasis (of alleles at more different loci)
generically dependent continuant;stratification rule;a stratification rule/criteria is a criteria used to determine population strata so that a stratification process implementing the rule can result in any member of the total population being assigned to one and only one stratum
generically dependent continuant;receiver operational characteristics curve;receiver operational characteristics curve is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold (aka cut-off point) is varied by plotting sensitivity vs (1 specificity)
generically dependent continuant;omega-squared;omega-squared is a effect size estimate for variance explained which is less biased than the eta-squared coefficient.
generically dependent continuant;true negative rate;specificity is a measurement datum qualifying a binary classification test and is computed by substracting the false positive rate to the integral numeral 1
generically dependent continuant;ma plot;an ma plot is a scatter plot of the log intensity ratios m = log 2(t/r) versus the average log intensities a = log 2(t*t)/2, where t and r represent the signal intensities in the test and reference channels respectively.
generically dependent continuant;galbraith plot;galbraith (radial) plot is a scatter plot which can be used in the meta-analytic context to examine the data for heterogeneity. for a fixed-effects model, the plot shows the inverse of the standard errors on the horizontal axis against the individual observed effect sizes or outcomes standardized by their corresponding standard errors on the vertical axis. radial plots were introduced by rex galbraith (1988a, 1988b, 1994). 
generically dependent continuant;rarefaction curve;a rarefaction curve is a graph used for estimating species richness in ecology studies
generically dependent continuant;probability distribution shape parameter;a probability distribution shape parameter is a data item which is set by the operator when selecting a parametric probability distribution and which dictates the way the profile but not the location or size of the distribution plot looks like.
generically dependent continuant;sidik-jonkman estimator;sidik-jonkman estimator is a data item computed to estimate heterogeneity parameter (estimate of between-study variance) in a random effect model for meta analysis.
generically dependent continuant;one dimensional cartesian coordinate system;a one dimensional cartesian coordinate system is a cartesian coordinate system which defines a one dimensional axis and which may be used to describe a one dimensional spatial region, i.e. a straight line. it is defined by a point o, the origin, a unit of length and the orientation for the one dimensional space.
generically dependent continuant;statistical model;a statistical model is an information content entity which is a formalization of relationships between variables in the form of mathematical equations. a statistical model describes how one or more random variables are related to one or more other variables. the model is statistical as the variables are not deterministically but stochastically related.
generically dependent continuant;loadings;in factor analysis, factor loadings express the relationship of each variable to the underlying factor.
generically dependent continuant;accuracy;in the context of binary classification, accuracy is defined as the proportion of true results (both true positives and true negatives) to the total number of cases examined (the sum of true positive, true negative, false positive and false negative). it can be understood as a measure of the proximity of measurement results to the true value.
generically dependent continuant;95% credible interval;in bayesian statistics context, a 95% credible interval is a credible interval which,given the data, includes the true parameter with probability of 95%.
generically dependent continuant;matthews correlation coefficient;matthews correlation coefficient (or mcc) is a correlation coefficient which is a measure of the quality of binary (two-class) classifications, introduced by biochemist brian w. matthews in 1975.
generically dependent continuant;helmert contrast;a helmert contrast is a contrast in which the coefficients for the helmert regressors compare each level with the average of the preceding ones
generically dependent continuant;heterogeneous first-order autoregressive covariance structure;this is an homogeneous structure, i.e. the variance along the main diagonal is constant. the covariances decline exponentially. it has only 2 parameters.
generically dependent continuant;additive genetic inheritance model;additive genetic model is a data item which refer to the contributions to the final phenotype from more than one gene, or from alleles of a single gene (in heterozygotes), that combine in such a way that the sum of their effects in unison is equal to the sum of their effects individually.
generically dependent continuant;binomial distribution;the binomial distribution is a discrete probability distribution which describes the probability of k successes in n draws with replacement from a finite population of size n. the binomial distribution is frequently used to model the number of successes in a sample of size n drawn with replacement from a population of size n. the binomial distribution gives the discrete probability distribution of obtaining exactly n successes out of n bernoulli trials (where the result of each bernoulli trial is true with probability p and false with probability q=1-p ) notation: b(n,p) the mean is n*p the variance is n*p*q
generically dependent continuant;matern function anisotropic covariance structure;given two sets of locations computes the matern cross covariance matrix for covariances among all pairings.
generically dependent continuant;absence of enrichment hypothesis;absence of depletion difference hypothesis is a hypothesis which assumes that the representation of an element significantly greater than a threshold does not exist.
generically dependent continuant;bayes factor;bayes factor is a ratio between 2 probabilities of observing data according 2 distinct models. it is used in bayes model selection to evaluate which model best explains the data. if k<0, the model used in the denominator term is supported, if k>1, the model used in the numerator term is supported. the bayes factor is about the plausibility of 2 different models
generically dependent continuant;measure of variation;measure of variation or statistical dispersion is a data item which describes how much a theoritical distribution or dataset is spread.
generically dependent continuant;coordinate system;in geometry, a coordinate system is a system which uses one or more numbers, or coordinates, to uniquely determine the position of a point or other geometric element on a manifold such as euclidean space.
generically dependent continuant;decile;a decile is a quantile where n=10 and which splits data into sections accrued of 10% of data, so the first decile delineates 10% of the data, the second decile delineates 20% of the data and the nineth decile, 90 % of the data
generically dependent continuant;first order autoregressive covariance structure;first order autoregressive covariance structure is a covariance structure where correlations among errors decline exponentially with distance
generically dependent continuant;root-mean-square standardized effect;root-mean-square standardized effect is a data item which denotes effect size in the context of analysis of variance and corresponds to the square root of the arithmetic average of p standardized effects (effects normalized to be expressed in standard deviation units).
generically dependent continuant;pareto type-iii probability distribution;the pareto(iii) distribution is a continous probability distribution which is described with a cumulative distribution function of the following form: f(x) = 1 [1 + ((x mu)/sigma)1/gamma] 1 for x > mu, sigma > 0, gamma > 0 and s =1. a is the location parameter, b is the scale parameter, g is the inequality parameter s is the shape parameter of value 1 the pareto iii distribution corresponds to a pareto type iv distribution where the shape parameter has a value of 1.
generically dependent continuant;biplot;biplots are a type of exploratory graph used in statistics, a generalization of the simple two-variable scatterplot. a biplot is constructed by using the singular value decomposition (svd) to obtain a low-rank approximation to a transformed version of the data matrix x, whose n rows are the samples (also called the cases, or objects), and whose p columns are the variables. the biplot was introduced by k. ruben gabriel (1971).
generically dependent continuant;prior probability distribution;a prior probability distribution is a probability distribution used as input to a bayesian model to represent a priori knowledge about a model parameter. along with the acquired/observed data, it is used to compute a posterior distribution according to the bayes theorem.
generically dependent continuant;homogeneity test objective;homogeneity testing objective is the objective of a data transformation to test a null hypothesis that two or more sub-groups of a population share the same distribution of a single categorical variable. for example, do people of different countries have the same proportion of smokers to non-smokers
generically dependent continuant;population mean;the population mean or distribution mean is a parameter of a probability distribution or population indicative of the data dispersion. for continous probabibility distribution, the population mean is computed using the probability density function, for discrete probability distributions, a mass density function is used instead. a population mean can be estimated by computing a sample mean
generically dependent continuant;x-axis;x-axis is a cartesian coordinate axis which is orthogonal to the y-axis and the z-axis
generically dependent continuant;michaelis-menten constant;the michaelis constant is the substrate concentration at which the reaction rate is at half-maximum, and is an inverse measure of the substrate's affinity for the enzyme as a small indicates high affinity, meaning that the rate will approach more quickly.[5] the value of is dependent on both the enzyme and the substrate, as well as conditions such as temperature and ph.
generically dependent continuant;log normal distribution;a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. thus, if the random variable {\displaystyle x} x is log-normally distributed, then {\displaystyle y=\ln(x)} y=\ln(x) has a normal distribution. likewise, if {\displaystyle y} y has a normal distribution, then {\displaystyle x=\exp(y)} x=\exp(y) has a log-normal distribution. a random variable which is log-normally distributed takes only positive real values. the distribution is occasionally referred to as the galton distribution or galton's distribution, after francis galton.
generically dependent continuant;wilk's lambda distribution;wilks' lambda distribution (named for samuel s. wilks), is a probability distribution used in multivariate hypothesis testing, especially with regard to the likelihood-ratio test and multivariate analysis of variance. it is a multivariate generalization of the univariate f-distribution, and generalizes the f-distribution in the same way that the hotelling's t-squared distribution generalizes student's t-distribution.
generically dependent continuant;sum contrast;the sum contrast is a contrast in which each coefficient compares the corresponding level of the factor to the average of the other levels
generically dependent continuant;spatial spherical geometric anisotropic covariance structure;spatial spherical geometric anisotropic covariance structure is a type of covariance structure characterized by its anisotropy, i.e., the variation of properties can be different in directions x and y, which is this case give spherical features.
generically dependent continuant;statistical test power;the statistical test power is data item which is about a statistical test and is obtained by subtracting the false negative rate (type ii error rate) to 1. the power of a statistical test is the probability that it will correctly lead to the rejection of a false null hypothesis (greene 2000). the statistical power is the ability of a test to detect an effect, if the effect actually exists (high 2000).
generically dependent continuant;2 step fagan nomogram;two-step fagan nomogram, which adds two extra axis between the lr axis that represents sensibility and specificity to calculate negative and positive likelihood ratios in the same nomogram
generically dependent continuant;altman box and whisker plot;altman box and whisker plot is a variation of tukey box and whisker plot which use the criteria of altman to create the 'whisker' of the plot.
generically dependent continuant;bayesian information criterion;bayesian information criterion or schwartz's bayesian information criterion is a criterion for model selection among a finite set of models. it is based, in part, on the likelihood function and it is closely related to the akaike information criterion (aic). given any two estimated models, the model with the lower value of bic is the one to be preferred. the bic is an increasing function of sigma e^2 and an increasing function of k. that is, unexplained variation in the dependent variable and the number of explanatory variables increase the value of bic. hence, lower bic implies either fewer explanatory variables, better fit, or both.
generically dependent continuant;kendall's correlation coefficient;kendall's correlation coefficient is a correlation coefficient between 2 ordinal variables (natively or following a ranking procedure) and may be used when the conditions for computing pearson's correlation are not met (e.g linearity, normality of the 2 continuous variables)
generically dependent continuant;bean plot;beanplot is a plot in which (one or) multiple batches ("beans") are shown. each bean consists of a density trace, which is mirrored to form a polygon shape. next to that, a one-dimensional scatter plot shows all the individual measurements, like in a stripchart. the name beanplot stems from green beans. the density shape can be seen as the pod of a green bean, while the scatter plot shows the seeds inside the pod.
generically dependent continuant;percentile;a percentile is a quantile which splits data into sections accrued of 1% of data, so the first percentile delineates 1% of the data, the second quartile delineates 2% of the data and the 99th percentile, 99 % of the data
generically dependent continuant;likelihood ratio;the likelihood ratio is a ratio which is formed by dividing the post-test odds with the pre-test odds in the context of a bayesian formulation
generically dependent continuant;box behnkens design;the box-behnken design is an independent quadratic design in that it does not contain an embedded factorial or fractional factorial design. in this design the treatment combinations are at the midpoints of edges of the process space and at the center. these designs are rotatable (or near rotatable) and require 3 levels of each factor. the designs have limited capability for orthogonal blocking compared to the central composite designs.
generically dependent continuant;r2 measure of linkage desequilibrium;r2 is a correlation coefficient which is computed over the frequency of 2 dichotomous variable and is used as a measure of linkage disequilibrium and as input data item to the creation of an ld plot
generically dependent continuant;group sequential design;group sequential design is a study design used in clinical trial settings in which interim analyses of the data are conducted after groups of patients are recruited. after each interim analysis, the trial may stop early if the evidence so far shows the new treatment is particularly effective or ineffective. such designs are ethical and cost-effective, and so are of great interest in practice.
generically dependent continuant;signal to noise ratio;signal to noise ratio is a measurement datum comparing the amount of meaningful, useful or interesting data (the signal) to the amount of irrelevant or false data (the noise). depending on the field and domain of application, different variables will be used to determinate a 'signal to noise ratio'. in statistics, the definition of signal to noise ratio is the ratio of the mean of a measurement to its standard deviation. it thus corresponds to the inverse of the coefficient of variation
generically dependent continuant;incidence;incidence is the ratio of the number of new cases of a disease divided by the number of persons at risk for the disease.
generically dependent continuant;banded toeplitz covariance structure;a banded toeplitz structure, defined by parameter q, can be viewed as a moving-average structure with order q-1.
generically dependent continuant;genetic inheritance model;genetic inheritance model is a data item defining the assumption used by a breeding value estimation method to consider when running the calculations.
generically dependent continuant;l'abbe plot;the l abb plot was introduced in 1987 in the context of meta-analyses of clinical trials with dichotomous (binary) outcomes, as a plot of observed risks in the treatment group against observed risks in the control group. another formulation is that it plots the event rate in the experimental (intervention) group against the event rate in the control group, as an aid to exploring the heterogeneity of effect estimates within a meta-analysis. it is diagram used in meta-analysis that compares the risks observed in the experimental and control arms of clinical trials. each trial is located in the space of a diagram where the sizes of the circles indicate the sizes of the trials. trials in which the experimental treatment had a higher risk than the control will be in the upper left of the plot. if risk in the both groups is the same the circle will fall on the line of equality. if the control treatment has a higher risk than the experimental treatment then the point will be in the lower right of the plot. it is often used as an indicator of heterogeneity and hence as an indicator of the likelihood that results from different trials can be validly combined. named after kristin l'abb . 
generically dependent continuant;blocking variable;a blocking variable is a independent variable which is used in a blocking process part of an experiment with the purpose of maximizing the signal coming from the main variable.
generically dependent continuant;studentized range distribution;the studentized range (q) distribution is a probability distribution used by the tukey honestly significant difference test. the distribution of the statistic [x (k)- x (1)]/(s/ n) where random samples of size n have been taken from k independent and identically distributed normal populations, with x (1) and x (k) being, respectively, the smallest and largest of the k sample means, and s2 being the pooled estimate of the common variance. this statistic is particularly used in multiple comparison tests.
generically dependent continuant;linear mixed model;a lnear mixed model is a mixed model containing both fixed effects and random effects and in which factors and covariates are assumed to have a linear relationship to the dependent variable. these models are useful in a wide variety of disciplines in the physical, biological and social sciences. they are particularly useful in settings where repeated measurements are made on the same statistical units (longitudinal study), or where measurements are made on clusters of related statistical units. because of their advantage in dealing with missing values, mixed effects models are often preferred over more traditional approaches such as repeated measures anova. fixed-effects factors are generally considered to be the variables whose values of interest are all represented in the data file. random-effects factors are variables whose values correspond to unwanted variation. they are useful when trying to understand variability in the dependent variable which was not anticipated and exceeds what was expected. linear mixed models also allow to specify specific interactions between factors, and allow the evaluation of the various linear effect that a particular combination of factor levels may have on a response variable. finally, linear mixed models allow to specify variance components in order to describe the relation between various random effects levels.
generically dependent continuant;hypergeometric distribution;hypergeometric distribution is a probability distribution that describes the probability of k successes in n draws from a finite population of size n containing k successes without replacement
generically dependent continuant;factor level combination;a factor level combination is one a possible sets of factor levels resulting from the cartesian product of sets of factor and their levels as defined in a factorial design
generically dependent continuant;restricted randomized design;a restricted randomized design is a kind of study design which uses randomization to allocate observation unit to treatment but where intuitively poor allocations of treatments to experimental units are avoided, while retaining the theoretical benefits of randomization. this is often the case when so-called 'hard to change' factors are used in an experimental design.
generically dependent continuant;absence of depletion hypothesis;absence of depletion difference hypothesis is a hypothesis which assumes that the representation of an element significantly less than a threshold does not exist.
generically dependent continuant;scaled identity covariance structure;the scaled identity covariance structure is a type of covariance structure which has constant variance. the assumption is that there is no correlation between any elements.
generically dependent continuant;presence of association hypothesis;a null hypothesis which states that a linkage exists between 2 categorical variables
generically dependent continuant;dichotomous variable;a dichotomous variable is a categorical variable which is defined to have only 2 categories or possible values
generically dependent continuant;tetrachoric correlation coefficient;a tetrachoric correlation coefficient is a polychoric correlation coefficient for 2 dichotomous variables used as proxy for correlation between 2 continuous latent variables.
generically dependent continuant;single factor design;a single factor design is a study design which declares exactly 1 independent variable
generically dependent continuant;spatial linear geometric anisotropic covariance structure;spatial linear geometric anisotropic covariance structure is a type of covariance structure characterized by its anisotropy, i.e., the variation of properties can be different in directions x and y, which is this case give linear features.
generically dependent continuant;model fixed effect term;a model fixed effect term is a model term which accounts for variation explained by an independent variable and its levels.
generically dependent continuant;negative likelihood ratio;the likelihood ratio of negative results is a ratio which is formed by dividing the difference between 1 and sensitivity of the test by the specificity value of a test.. this can be expressed also as dividing the probability of a person who has the disease testing negative by the probability of a person who does not have the disease testing negative.
generically dependent continuant;percentage of variance;the percentage of variance is an output of principal component analysis which is obtained by forming the ratio of an eigenvalue divided by the sum of all eigenvalues. this produces a "percentage of variance" for each eigenvector.
generically dependent continuant;confidence interval;a confidence interval is a data item which defines an range of values in which a measurement or trial falls corresponding to a given probability.
generically dependent continuant;maximum value;maximum value is a data item which denotes the largest value found in a dataset or resulting from a calculation.
generically dependent continuant;regression model;a regression model is a statistical model used in a type of analysis knowns as regression analysis, whereby a function is used to determine the relation between a response variable and an independent variable , with a set of unknown parameters.
generically dependent continuant;goodness of fit hypothesis;goodness of fit hypothesis is a null hypothesis stating that the distribution computed from the sample population fits a theoretical distribution or that a dataset can be correctly explained by a model
generically dependent continuant;relative risk;relative risk is a measurement datum which denotes the risk of an 'event' relative to an 'exposure'. relative risk is calculated by forming the ratio of the probability of the event occurring in the exposed group versus the probability of this event occurring in the non-exposed group.
generically dependent continuant;coefficient of variation;the coefficient of variation is a normalized measure of dispersion of a probability distribution of frequency distribution.
generically dependent continuant;plackett-burman design;plackett-burman design is a type of study design optimizing multifactorial experiments characterized by their parsimony and economy with the run number a multiple of 4 (rather than a power of 2). plackett-burman design is often used for screening experiments where the main effect is often heavily confounded with two-factor interactions. this type of design is very useful for economically detecting large main effects, assuming all interactions are negligible when compared with the few important main effects.
generically dependent continuant;99% confidence interval;a confidence interval which covers 99% of the sampling distribution, meaning that there is a 1% risk of false positive (type i error)
generically dependent continuant;huynh-feldt covariance structure;a form of covariance structure used to provide analysis ground s in the context of repeated measures datasets (longitudinal, time series)
generically dependent continuant;fagan nomogram;fagan nomogram is a graph plotting pre-test probabilities, likelyhood ratios and post-test probabilities on 3 parallel axis. the plot was first proposed by fagan in 1975 as a way to visualize baye's theorem data where p(d) is the probability that the patient has the disease before the test. p(d|t) is the probability that the patient has the disease after the test result. p(t|d) is the probability of the test result if the patient has the disease, and p(t|d ) is the probability of the test result if the patient does not have the disease. with this terminology the usefulness of both positive and negative test results can be assessed. a line drawn from p(d) on the right through the ratio of p(t|d) to p(t|d ) gives p(d|t) on the left of the nomogram.
generically dependent continuant;rational quadratic anisotropic covariance structure;the rational quadratic covariance function is used in spatial statistics, geostatistics, machine learning, image analysis, and other fields where multivariate statistical analysis is conducted on metric spaces. it is commonly used to define the statistical covariance between measurements made at two points that are d units distant from each other. since the covariance only depends on distances between points, it is stationary. if the distance is euclidean distance, the rational quadratic covariance function is also isotropic.
generically dependent continuant;statistical error;statistical error is an data item denoting the amount by which an observation differs from the expected value, being based on the whole statistical population from which the statistical unit was chosen randomly
generically dependent continuant;treatment contrast;treatment contrast is a contrast which allows to test how linear model coefficients of categorical variables are interpreted in case where the first level (aka, the baseline) is included into the intercept and all subsequent levels have a coefficient that represents their difference from the baseline.
generically dependent continuant;kurtosis;kurtosis is a data item which denotes the degree of peakedness of a distribution. it is defined as a normalized form of the fourth central moment of a distribution.
generically dependent continuant;bernoulli distribution;bernoulli distribution is a binomial distribution where the number of trials is equal to 1. notation: b(1,p) the mean is p the variance is p*q
generically dependent continuant;contrast weight;a contrast weight is a coefficient which multiplies a group mean, part of a linear combinaison defining a constrast as a weighted sum of group means, giving a 'weight' to a specific group mean hence the name.
generically dependent continuant;g matrix;a matrix of relationships among a group of individuals, which can be used to predict breeding values, to manage inbreeding and in genetic conservation. it can be calculated from the pedigree, but it is also possible to calculate the relationship matrix from genotypes at genetic markers such as single-nucleotide polymorphisms (snps). elements of the genomic relationship matrix are estimates of the realized proportion of the genome that two individuals share, whereas the pedigree-derived relationship matrix is the expectation of this proportion.
generically dependent continuant;spear box and whisker plot;spear box and whisker plot is a variation of tukey box and whisker plot which use the criteria of spear to create the 'whisker' of the plot.
generically dependent continuant;two dimensional cartesian coordinate system;a 2 dimensional cartesian coordinate system is a cartesian coordinate system which defines 2 orthogonal one dimensional axes and which may be used to describe a 2 dimensional spatial region.
generically dependent continuant;cartesian spatial coordinate origin;a cartesian spatial coordinate datum chosen as a fixed point of reference in a spatial region.
generically dependent continuant;50% confidence interval;a confidence interval which covers 50% of the sampling distribution, meaning that there is a 50% risk of false positive (type i error)
generically dependent continuant;random variable;a random variable (or aleatory variable or stochastic variable) in probability and statistics, is a variable whose value is subject to variations due to chance (i.e. randomness, in a mathematical sense)
generically dependent continuant;corrected akaike information criterion;corrected akaike information criteria is a modified version of the akaike information criterion.
generically dependent continuant;q-q plot;q-q plot or quantile-quantile plot is the output of a graphical method for comparing two probability distributions by plotting their quantiles against each other
generically dependent continuant;negative binomial distribution;negative binomial probability distribution is a discrete probability distribution of the number of successes in a sequence of bernoulli trials before a specified (non-random) number of failures (denoted r) occur. the negative binomial distribution, also known as the pascal distribution or p lya distribution, gives the probability of r-1 successes and x failures in x+r-1 trials, and success on the (x+r)th trial.
generically dependent continuant;standard error of estimate;it is a measure of how precise is an estimate of the statistical parameter is. standard error is the estimated standard deviation of an estimate. it measures the uncertainty associated with the estimate. compared with the standard deviations of the underlying distribution, which are usually unknown, standard errors can be calculated from observed data.
generically dependent continuant;poisson distribution;poisson distribution is a probability distribution used to model the number of events occurring within a given time interval. it is defined by a real number ( ) and an integer k representing the number of events and a function. the expected value of a poisson-distributed random variable is equal to and so is its variance.
generically dependent continuant;pareto type-ii probability distribution;the pareto type-ii probability distribution is a continuous probability distribution which is defined by a probability density function characterized by 2 parameters, alpha and lambda, 2 real, strictly positive numbers. alpha is known as the shape parameter while lambda is known as the scale parameter. the function defines the probably of a continous random variable according to the following: p(x) = {\alpha \over \lambda} \left[{1+ {x \over \lambda}}\right]^{-(\alpha+1)}, \qquad x \geq 0, 
generically dependent continuant;probability distribution scale parameter;a probability distribution scale parameter is a measure of variation which is set by the operator when selecting a parametric probability distribution and which defines how spread the distribution is. the larger the value of the scale parameter is, the more spread out the distribution.
generically dependent continuant;precision;precision or positive predictive value is defined as the proportion of the true positives against all the positive results (both true positives and false positives)
generically dependent continuant;hardy-weinberg equilibrium hypothesis;hardy-weinberg equilibrium hypothesis is a good of fit hypothesis which states that allele and genotype frequencies in a population will remain constant from generation to generation in the absence of other evolutionary influences (non-random mating, mutation, selection, genetic drift, gene flow and meiotic drive).
generically dependent continuant;model parameter;a model parameter is a data item which is part of a model and which is meant to characterize an theoritecal or unknown population. a model parameter may be estimated by considering the properties of samples presumably taken from the theoritecal population
generically dependent continuant;symmetric distribution;probability distribution which has no skew so its skewness=0
process;statistical model term testing;a statistical model term testing is a data transformation that accounts for the evaluation of a component of a statistical model or model term.
process;barnard's test;barnard's test is an exact statistical test used to determine if there are nonrandom associations between two categorical variables. it was developed in 1949 by barnard and is a test which is, most times, more powerfull that the fisher exact test
process;trait-specific relationship matrix best linear unbiaised prediction;a data transformation which calculate estimates of genomic estimated breeding values (gebvs) on an animal or plant model utilizing trait-specific marker information.
process;genetic association study;a genetic association study is a kind of study whose objective is to detect associations between phenotypes, between a phenotype and a genetic polymorphism or between two genetic polymorphisms.
process;newman-keuls test post-hoc analysis;the newman keuls or student newman keuls (snk) method is a stepwise multiple comparisons procedure used to identify sample means that are significantly different from each other. it was named after student (1927), d. newman, and m. keuls. this procedure is often used as a post-hoc test whenever a significant difference between three or more sample means has been revealed by an analysis of variance (anova). the newman keuls method is similar to tukey's range test as both procedures use studentized range statistics.compared to tukey's range test, the newman keuls method is more powerful but less conservative.
process;reproducing kernel hilbert space procedure;a data transformation that produces a reproducing kernel hilbert space (or rkhs), which is a hilbert space of functions in which point evaluation is a continuous linear functional.
process;restricted maximum likelihood estimation;restricted maximum likelihood estimation is a kind of maximum likelihood estimation data transformation which estimates the variance components of random-effects in univariate and multivariate meta-analysis. in contrast to 'maximum likelihood estimation', reml can produce unbiased estimates of variance and covariance parameters.
process;restricted randomization;restricted randomization is a kind of randomization which is used or occured when hard to change factors exist in a study design. in other words, when complete randomization is not possible, a case of restricted randomization exists, for instance in the case of split-plot design. restricted randomization allows intuitively poor allocations of treatments to experimental units to be avoided, while retaining the theoretical benefits of randomization. restricted randomization can also result from an unplanned event and is then something that should be avoided. randomizer r package can be used to detect such events and assess the quality of randomization process. 
process;generalized extreme studentized deviate test;the extreme studentized deviate test is a statistical test used to detect outliers in a univariate data set assumed to come from a normally distributed population. the esd test differs from the grubbs' test and the tietjen-moore test in the sense that it contains built-in correction for multiple testing.
process;hommel false discovery rate correction;a data transformation process in which the hommel p-value procedure is applied with the aim of correcting false discovery rate
process;meta analysis by hartung-knapp-sidik-jonkman method;a random effect meta analysis procedure defined by hartung and knapp and by sidik and jonkman which performs better than dersimonian and laird approach, especially when there is heterogeneity and the number of studies in the meta-analysis is small.
process;meta analysis by dersimonian and leard method;a meta analysis which relies on the computation of the dersimonian and leard estimator as a measure of heterogeneity over a set of studies.
process;test of association between categorical variables;linkage between 2 categorical variable test is a statistical test which evaluates if there is an association between a predictor variable assuming discrete values and a response variable also assuming discrete values
process;best linear unbiased predictor;best linear unbiased prediction is a data transformation which predicts <tdb> under the assumption that the variable(s) under consideration have a random effect
process;factor analysis;factor analysis is a dimension reduction data transformation that is used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. factor analysis is related to principal component analysis (pca), but the two are not identical. both pca and factor analysis aim to reduce the dimensionality of a set of data, but the approaches taken to do so are different for the two techniques. factor analysis is clearly designed with the objective to identify certain unobservable factors from the observed variables, whereas pca does not directly address this objective. at best, pca provides an approximation to the required factors.
process;partial least square discriminant analysis;a version of pls used for classification, where the input y-block are group labels (categorical variable) rather than a continuous variable
process;ranking;ranking is a data transformation which turns a non-ordinal variable into a ordinal variable by sorting the values of the input variable and replacing their value by their position in the sorting result
process;brewer sampling;brewer's sampling is a statistical sampling method which was proposed by brewer in 1975 and uses unequal probabibility sampling technique
process;wald test;the wald test is statistical test which computes a wald chi-squared test for 1 or more coefficients, given their variance-covariance matrix. the wald test (also called the wald chi-squared test) is a way to find out if explanatory variables in a model are significant. significant means that they add something to the model. variables that add nothing can be deleted without affecting the model in any meaningful way
process;bayes c pi;bayes c pi is a data transformation used to compute estimated breeding values using a bayesian model and which assesses the snp effect using montecarlo markov chain methods. bayes c pi treats the prior probability that a snp has zero effect as unknown. the method was devised to address short comings of bayes a and bayes b approaches
process;two sample t-test with unequal variance;welch t-test is a two sample t-test used when the variances of the 2 populations/samples are thought to be unequal (homoskedasticity hypothesis not verified). in this version of the two-sample t-test, the denominator used to form the t-statistics, does not rely on a 'pooled variance' estimate.
process;conover-iman test of multiple comparisons using rank sums;conover-iman test for stochastic dominance is a stastical test for multiple group comparisons and reports the results among multiple pairwise comparisons after a kruskal-wallis test for stochastic dominance among k groups (kruskal and wallis, 1952). the interpretation of stochastic dominance requires an assumption that the cdf of one group does not cross the cdf of the other. the null hypothesis for each pairwise comparison is that the probability of observing a randomly selected value from the first group that is larger than a randomly selected value from the second group equals one half. this null hypothesis corresponds to that of the wilcoxon-mann-whitney rank-sum test. like the rank-sum test, if the data can be assumed to be continuous, and the distributions are assumed identical except for a difference in location, conover-iman test may be understood as a test for median difference. conover.test accounts for tied ranks. the conover-iman test is strictly valid if and only if the corresponding kruskal-wallis null hypothesis is rejected.
process;pearson's chi square test of goodness of fit;pearson's chi-squared test for goodnes of fit is a statistical null hypothesis test which is used to either evaluate goodness of fit of dataset to a chi-squared distribution
process;pls1;a partial least square regression applied when there is only one variable in y (the matrix of response variables), or it is desirable to model and optimize separately the performance of each of the variables in y. this case is usually referred to as pls1 regression (j = 1).
process;convenience sampling;convenience sampling (also known as grab sampling, accidental sampling, or opportunity sampling) is a type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand. this type of sampling is most useful for pilot testing.
process;cluster sampling;cluster sampling is a sampling plan used when mutually homogeneous yet internally heterogeneous groupings are evident in a statistical population.
process;simple random sampling;simple random sampling is a statistical sampling process which creates a sample of a size n entirely by chance. in such process, each unit has the same probability of being selected. depending on the size of the population being sampled, the sampling process may be done with or without replacement
process;dna microarray hybridization;a dna microarray hybridization is an assay relying on nucleic acid hybridization , which uses a dna microarray device and a nucleic acid as input. it precedes a data acquisition process
process;dixon q test;dixon test is a statistical test used to detect outliers in a univariate data set assumed to come from a normally distributed population.
process;weighted least squares estimation;the weighted least squares estimation is a model parameter estimation for a linear regression model with errors that independent but have heterogeneous variance. difficult to use use in practice, as weights must be set based on the variance which is usually unknown. if true variance is known, it is the best linear unbiased estimation (blue) method under these assumptions, uniformly minimum-variance unbiased estimator (umvue) with addition of a gaussian assumption.
process;chain-referral sampling;snowball sampling (or chain sampling, chain-referral sampling, referral sampling) is a non-probability sampling technique where existing study subjects recruit future subjects from among their acquaintances. thus the sample group is said to grow like a rolling snowball.
process;line intercept sampling;line intercept sampling is a sampling process by which an element in a spatial region is included in a sample if it is intersected by a line chosen by the operator.
process;continuous variable discretization;discretization as a processing converting a continuous variable into a polychotomous variable by concretizing a set of discretization rules
process;tietjen-moore test for outliers;tietjen-moore test for outlier is a statistical test used to detect outliers and corresponds to a generalization of the grubb's test, thus allowing detection of more than one outlier in a univariate data set assumed to come from a normally distributed population. if testing for a single outlier, the tietjen-moore test is equivalent to the grubbs' test.
process;permutation numbering;permutation numbering is a data tranformation allowing to count the number of possible permutations of elements in a set of size n, each element occurring exactly once. this number is factorial n.
process;breusch-pagan test;breusch-pagan test is a statistical test which computes a score test of the hypothesis of constant error variance against the alternative that the error variance changes with the level of the response (fitted values), or with a linear combination of predictors. 
process;paired t-test;paired t-test is a statistical test which is specifically designed to analysis differences between paired observations in the case of studies realizing repeated measures design with only 2 repeated measurements per subject (before and after treatment for example)
process;hotelling-lawley trace test;"the lawley hotelling trace is used to test the equality of mean vectors of k p variate normal distributions with common but unknown covariance matrix. the explicit form of the null distribution of t$ {0}^{2}$equation image is the f distribution. the asymptotic null distribution is the chi square distribution. the power function of the test is described and its power is compared with the likelihood ratio test. "
process;bayes r;bayes r is a data transformation used in the context of estimating breeding value, which relies on a bayesian model to compute 'genomic estimated breeding values'. in contrast to bayes b methods, the new method assumes that the true snp effects are derived from a series of normal distributions, the first with zero variance, up to one with a variance of approximately 1% of the genetic variance.
process;non-parametric test;a statistical test which makes no assumption about the underlying data distribution
process;probit regression for analysis of polychotomous dependent variable;probit regression model is a model which attempts to explain data distribution associated with *dichotomous* response/dependent variable in terms of values assumed by the independent variable uses a function of predictor/independent variable(s): the function used in this instance of regression modeling is the probit function aka the quantile function, i.e., the inverse cumulative distribution function (cdf), associated with the standard normal distribution.
process;systematic sampling;systematic sampling is a process for collecting samples and assembling a statistical sample using a system or method (.e.g unequal probabilities, without replacement, fixed sample size), as opposed to a random sampling.
process;yuen t-test with trimmed means;the yuen's t-test is a two sample t-test with populations of unequal variance which provides a more robust t-test procedure under normal distribution and long tailed distributions. the test computes a t statistics not using 'arithmetic means' but using 'trimmed means' instead as well as winsorized variances.
process;cartesian product 2 sets;self explanatory
process;hardy-weinberg equilibrium testing;hardy-weinberg equilibrium test is a statistical test which aims to evaluate if a population's proportion of allele is stable or not. it is used as means of quality control to evaluate possibility of genotyping error or population structure.
process;pearson's chi square test of independence between categorical variables;pearson's chi-squared test is a statistical null hypothesis test which is used to either evaluate goodness of fit of dataset to a chi-squared distribution or used to test independence of 2 categorical variables (ie absence of association between those variables).
process;feasible generalized least squares estimation;the feasible generalized least squares estimation is a model parameter estimation which is a practical implementation of generalised least squares, where the covariance of the errors is estimated from the residuals of the regression model, providing the information needed to whiten the data and model. each successive estimate of the whitening matrix improves the estimation of the regression parameters, which in turn are used to compute residuals and update the whitening matrix.
process;grubbs' test;grubbs' test is a statistical test used to detect one outlier in a univariate data set assumed to come from a normally distributed population.
process;mcnemar test;mcnemar's test is a statistical test used on paired nominal data. it is applied to 2 2 contingency tables with a dichotomous trait, with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal (that is, whether there is "marginal homogeneity"). it is named after quinn mcnemar, who introduced it in 1947. an application of the test in genetics is the transmission disequilibrium test for detecting linkage disequilibrium
process;two tailed test;a two tailed test is a statistical test which assess the null hypothesis of absence of difference assuming a symmetric (not skewed) underlying probability distribution by allocating half of the significance level selected to each of the direction of change which could explain a difference (for example, a difference can be an excess or a loss).
process;complete randomization;complete randomization is a group randomization where experimental units are randomly assigned to the entire set of groups defined by the experimental treatments.
process;f-test;an f-test is a statistical test which evaluates that the computed test statistics follows an f-distribution under the null hypothesis. the f-test is sensitive to departure from normality. f-test arise when decomposing the variability in a data set in terms of sum of squares.
process;sphericity test;a sphericity test is a null hypothesis statistical testing procedure which posits a null hypothesis of equality of the variances of the differences between levels of the repeated measures factor
process;o'brien-flemming boundary analysis;the o'brien-flemming boundary analysis is a kind of interim-analysis method implemented by o'brien and flemming to account for the as all frequentist methods of the same type, it focuses on controlling the type i error rate as the repeated hypothesis testing of accumulating data increases the type i error rate of a clinical trial.
process;linear regression for analysis of continuous dependent variable;linear regression model is a model which attempts to explain data distribution associated with response/dependent variable in terms of values assumed by the independent variable uses a linear function or linear combination of the regression parameters and the predictor/independent variable(s). linear regression modeling makes a number of assumptions, which includes homoskedasticity (constance of variance)
process;metropolis hastings sampling;the metropolis hastings algorithm is a markov chain monte carlo (mcmc) method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult.
process;pocock boundary analysis;the pocock boundary analysis gives a p-value threshold for each interim analysis which guides the data monitoring committee on whether to stop the trial. the boundary used depends on the number of interim analyses. the pocock boundary is simple to use in that the p-value threshold is the same at each interim analysis. the disadvantages are that the number of interim analyses must be fixed at the start and it is not possible under this scheme to add analyses after the trial has started. another disadvantage is that investigators and readers frequently do not understand how the p-values are reported: for example, if there are five interim analyses planned, but the trial is stopped after the third interim analysis because the p-value was 0.01, then the overall p-value for the trial is still reported as <0.05 and not as 0.01. as all frequentist methods of the same type, it focuses on controlling the type i error rate as the repeated hypothesis testing of accumulating data increases the type i error rate of a clinical trial.
process;breeding value estimation using pedigree data;breeding value estimation is a data transformation process aiming at computing breeding value estimates of an organism given a set of pedigree information.
process;repeated measure analysis;repeated measure analysis is a kind of data transformation which deals with signals measured in the same experimental units at different times and, possibly, under different conditions over a period of time. data produced by longitudinal studies qualify for such analysis. since measurements are made on the same experimental units a number of times, they are likely to be correlated. repeated measure analysis usually takes into consideration the possibility of correlation with time. it does so by specifying covariance structure in the analysis
process;binomial logistic regression for analysis of dichotomous dependent variable;binomial logistic regression model is a model which attempts to explain data distribution associated with *dichotomous* response/dependent variable in terms of values assumed by the independent variable uses a function of predictor/independent variable(s): the function used in this instance of regression modeling is logistic function.
process;sampling from a probability distribution;sampling from a probability distribution is a data transformation which aims at obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult.
process;panel sampling;panel sampling is the method of first selecting a group of participants through a random sampling method and then asking that group for (potentially the same) information several times over a period of time. therefore, each participant is interviewed at two or more time points. each period of data collection is called a "wave". the method was developed by sociologist paul lazarsfeld in 1938 as a means of studying political campaigns.
process;exact binomial test;a binomial test is a statistical hypothesis test which evaluates if the observations made about a bernoulli experiment , that is an experiment which tests the statistical significance of deviations from a theoretically expected distribution (the binomial distribution) of observations into 2 categories. it is a goodness of fit test.
process;sampling distribution estimation by bootstrapping;bootstrapping is the practice of estimating properties of an estimator (such as its variance) by measuring those properties when sampling from an approximating distribution.
process;cochran-mantel-haenzel test for repeated tests of independence;cochran-mantel-haenzel test for repeated tests of independence is a statitiscal test which allows the comparison of two groups on a dichotomous/categorical response. it is used when the effect of the explanatory variable on the response variable is influenced by covariates that can be controlled. it is often used in observational studies where random assignment of subjects to different treatments cannot be controlled, but influencing covariates can. the null hypothesis is that the two nominal variables that are tested within each repetition are independent of each other. so there are 3 variables to consider: two categorical variables to be tested for independence of each other, and the third variable identifies the repeats.
process;simpls;a novel algorithm for partial least squares (pls) regression, simpls, is proposed which calculates the pls factors directly as linear combinations of the original variables. the pls factors are determined such as to maximize a covariance criterion, while obeying certain orthogonality and normalization restrictions. this approach follows that of other traditional multivariate methods. the construction of deflated data matrices as in the nonlinear iterative partial least squares (nipals)-pls algorithm is avoided. for univariate y simpls is equivalent to pls1 and closely related to existing bidiagonalization algorithms. this follows from an analysis of pls1 regression in terms of krylov sequences. for multivariate y there is a slight difference between the simpls approach and nipals-pls2. in practice the simpls algorithm appears to be fast and easy to interpret as it does not involve a breakdown of the data sets. the acronym simpls comes from 'straightforward implementation of a statistically inspired modification of the pls method'
process;ancova;ancova or analysis of covariance is a data transformation which evaluates if population means of a dependent variable are equal across levels of a categorical independent variables while controlling for the effects of other continuous variable s, known as covariates. therefore, when performing ancova, we are adjusting the dependent variable means to what they would be if all groups were equal on the covariates. it augments the anova model with one or more additional quantitative variables, called covariates, which are related to the response variable. the covariates are included to reduce the variance in the error terms and provide more precise measurement of the treatment effects. ancova is used to test the main and interaction effects of the factors, while controlling for the effects of the covariate
process;bayes b;bayes b is a data transformation used in the context of estimating breeding value, which relies on a bayesian model, treats the prior probability that a snp has zero effect to a set value (i.e >0) and uses a mixture distribution.
process;non-iterative partial least squares;a data transformation which finds principal component by applying non-linear iterative partial least squares algorithm
process;ridge regression best linear unbiaised predictor;rr-blup is a data transformation used in the context of estimating breeding value using a bayesian ridge regression. it can be obtained from bayes b procedure by setting pi parameter to zero ( ) and assuming that all the markers have the same variance.
process;meta analysis;meta-analysis is a data transformation which uses the effect size estimates from several independent quantitative scientific studies addressing the same question in order to assess finding consistency.
process;subject pairing;a subject pairing is a planned process which executes a pairing rule and results in the creation of sets of 2 subjects meeting the pairing criteria
process;bayes a;bayes a is a data transformation used in the context of estimating breeding value, which relies on a bayesian model and treats the prior probability that a snp has zero effect as unknown (i.e =0)
process;cochran's q test for heterogeneity;cochran's q test is a statistical test used for unreplicated randomized block design experiments with a binary response variable and paired data. in the analysis of two-way randomized block designs where the response variable can take only two possible outcomes (coded as 0 and 1), cochran's q test is a non-parametric statistical test to verify whether k treatments have identical effects. 
process;best linear unbiased estimator;best linear unbiased estimator
process;genome-wide association study;genome wide association study is a kind of study whose objective is to detect association between genetic markers (snp or otherwise) accross the genome and a trait which may be a disease or another phenotype (e.g. trait of agronomic relevance in animal or plant studies). genome wide association study compare the allele frequencies in 2 populations, one free of the trait used as control, the other one showing the trait use as 'case'. gwas studies implement case-control design
process;homoskedasticity test;an homoskedasticity test is a statistical test aiming at evaluate if the variances from several random samples are similar
process;within subject comparison statistical test;within subject comparison statistical test is a kind of statistical test which evaluates if a change occurs within one experimental unit over time following a treatment or an event
process;holm false discovery rate correction;a data transformation process in which the holm p-value procedure is applied with the aim of correcting false discovery rate
process;tarone's test for homogeneity of odds ratio;tarone's test for homogeneity of odds ratio is a statistical test which evaluates the null hypothesis that odds ratio are homogeneous
process;random forest procedure;random forest procedure is a type of data transformation used in classification and statistical learning using regression. the random forest procedure is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset (it operates by constructing a multitude of decision trees at training time) and use averaging to improve the predictive accuracy and control over-fitting. the sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=true (default). the random forest procedure outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.
process;maximum likelihood estimation;maximum likelihood estimation (mle) is a method of estimating the parameters of a statistical model, given observations. mle attempts to find the parameter values that maximize the likelihood function, given the observations. the method of maximum likelihood is based on the likelihood function, {\displaystyle {\mathcal {l}}(\theta \,.x)} {\displaystyle {\mathcal {l}}(\theta \,.x)}. we are given a statistical model, i.e. a family of distributions {\displaystyle \{f(\cdot \,.\theta )\mid \theta \in \theta \}} {\displaystyle \{f(\cdot \,.\theta )\mid \theta \in \theta \}}, where {\displaystyle \theta } \theta denotes the (possibly multi-dimensional) parameter for the model. the method of maximum likelihood finds the values of the model parameter, {\displaystyle \theta } \theta , that maximize the likelihood function, {\displaystyle {\mathcal {l}}(\theta \,.x)} {\displaystyle {\mathcal {l}}(\theta \,.x)}. i
process;statistical model selection;a statistical model selection is a data transformation which is based on computing a relative quality value in order to evaluate and select which model best explains data.
process;two sample t-test with equal variance;two sample t-test is a null hypothesis statistical test which is used to reject or accept the hypothesis of absence of difference between the means over 2 randomly sampled populations. it uses a t-distribution for the test and assumes that the variables in the population are normally distributed and with equal variances.
process;transmission disequilibrium test;the transmission disequilibrium test is a statistical test for genetic linkage between genetic marker and a trait in families. the test is robust to population structure.
process;pls2;a partial least square regression applied to a multivariate response variable.
process;gibbs sampling;gibbs sampling or a gibbs sampler is a markov chain monte carlo (mcmc) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult.
process;interim analysis;interim analysis is a data transformation used to analyzed studies implementing a group-sequential design, to evaluate and interpret the accumulating information during a clinical trial. it means that the analysis of data that is conducted before full data collection has been completed. clinical trials are unusual in that enrollment of patients is a continual process staggered in time. this means that if a treatment is particularly beneficial or harmful compared to the concurrent placebo group while the study is on-going, the investigators are ethically obliged to assess that difference using the data at hand and to make a deliberate consideration of terminating the study earlier than planned.
process;partial least square regression;partial least squares regression (pls regression) is a data transformation that bears some relation to principal components regression. instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. because both the x and y data are projected to new spaces, the pls family of methods are known as bilinear factor models. partial least squares discriminant analysis (pls-da) is a variant used when the y is categorical. pls is used to find the fundamental relations between two matrices (x and y), i.e. a latent variable approach to modeling the covariance structures in these two spaces. a pls model will try to find the multidimensional direction in the x space that explains the maximum multidimensional variance direction in the y space. pls regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among x values. by contrast, standard regression will fail in these cases (unless it is regularized). partial least squares was introduced by the swedish statistician herman o. a. wold, who then developed it with his son, svante wold. an alternative term for pls (and more correct according to svante wold[1]) is projection to latent structures, but the term partial least squares is still dominant in many areas. although the original applications were in the social sciences, pls regression is today most widely used in chemometrics and related areas. it is also used in bioinformatics, sensometrics, neuroscience and anthropology.
process;statistical test power analysis;a stastical test power analysis is a data transformation which aims to determine the size of a statistical sample required to reach a desired significance level given a particular statistical test
process;pillai's trace test;"pillai proposed the trace test for the following three tests: (a) equality of mean vectors of lp variate normal distributions with the common but unknown covariance matrix, (b) independence between two sets of variates distributed jointly as a normal distribution with unknown mean vector, and (c) equality of covariance matrices of two p variate normal distributions with unknown mean vectors."
process;iteratively reweighted least squares estimation;the iteratively reweighted least squares estimation is a model parameter estimation which is a practical implementation of weighted least squares, where the heterogeneous variances of the errors are estimated from the residuals of the regression model, providing an estimate for the weights. each successive estimate of the weights improves the estimation of the regression parameters, which in turn are used to compute residuals and update the weights
process;acute toxicity study;acute toxicity study is an investigation which use interventions organized according to a factorial design and a parallel group design to observe the effect of use of high dose xenobiotics in animal models or cellular models
process;satterthwaite degree of freedom approximation;satterthwaite degree of freedom approximation is a type of degree of freedom approximation which is used to estimate an effective degrees of freedom for a probability distribution formed from several independent normal distributions where only estimates of the variance are known. it was originally developed by statistician franklin e. satterthwaite.
process;woolf's test;woolf's test is a statistical test which evaluates the null hypothesis that odds ratio are the same accross all strata of population under investigation
process;repeated measure anova;repeated measure anova is a kind of anova specifically developed for non-independent observations as found when repeated measurements on the sample experimental unit. repeated measure anova is sensitive to departure from normality (evaluation using bartlett's test), more so in the case of unbalanced groups (i.e. different sizes of sample populations). departure from sphericity (evaluation using mauchly'test) used to be an issue which is now handled robustly by modern tools such as r's lme4 or nlme, which accommodate dependence assumptions other than sphericity.
process;assess statistical evidence;the process of using statistical analysis for interpreting and communicating "what the data say".
process;scheffe test;the scheffe test is a data transformation which evaluates all possible contrasts and adjusting the levels significance by accounting for multiple comparison. the test is therefore conservative. confidence intervals can be constructed for the corresponding linear regression. it was developped by american statistician henry scheffe in 1959.
process;one-way anova;one-way anova is an analysis of variance where the different groups being compared are associated with the factor levels of only one independent variable. the null hypothesis is an absence of difference between the means calculated for each of the groups. the test assumes normality and equivariance of the data.
process;degree of freedom approximation;an estimate of the number of degrees of freedom.
process;dunn s multiple comparison test;dunn s multiple comparison test is a post hoc (i.e. it s run after an anova) non parametric test (a distribution free test that doesn t assume your data comes from a particular distribution). it is one of the least powerful of the multiple comparisons tests and can be a very conservative test especially for larger numbers of comparisons. the dunn is an alternative to the tukey test when you only want to test for differences in a small subset of all possible pairs. for larger numbers of pairwise comparisons, use tukey s instead. use dunn s when you choose to test a specific number of comparisons before you run the anova and when you are not comparing to controls. if you are comparing to a control group, use the dunnett test instead.
process;improved kernel pls;improved kernel pls is a data transformation which implement a very fast kernel algorithm for updating pls models in a recursive manner and for exponentially discounting past data.
process;meta analysis by hunter-schmidt method;a meta analysis which relies on the computation of the hunter and schmidt estimator as a measure of heterogeneity over a set of studies by considering the weighted mean of the raw correlation coefficient. hunter and schmidt developed what is commonly termed validity generalization procedures (schmidt and hunter, 1977). these involve correcting the effect sizes in the meta-analysis for sampling, and measurement error and range restriction.
process;fisher's exact test;fisher's exact test is a statistical test used to determine if there are nonrandom associations between two categorical variables.
process;yate's corrected chi-squared test;yate's corrected chi-squared test is a statistical test which is used to test the association/linkage/independence of 2 dichotomous variables while introducing a correction for using the continous chi-squared distribution for the test. to reduce the error in approximation, frank yates, an english statistician, suggested a correction for continuity that adjusts the formula for pearson's chi-squared test by subtracting 0.5 from the difference between each observed value and its expected value in a 2 2 contingency table. this reduces the chi-squared value obtained and thus increases its p-value.
process;data imputation;data imputation is a data transformation process whereby missing data is replaced with an estimated value for the missing element. the substituted values are intended to create a data record that does not fail edits. various methods may be used to produce these substituted values.
process;cartesian product;a cartesian product is a data transformation which operates on a n sets to produce a set of all possible ordered n-tuples where each element of the tuple comes from a set
process;wilcoxon signed rank test;the wilcoxon signed rank test is a statistical test which tests the null hypothesis that the median difference between pairs of observations is zero. this is the non-parametric analogue to the paired t-test, and should be used if the distribution of differences between pairs may be non-normally distributed. the procedure involves a ranking, hence the name. the absolute value of the differences between observations are ranked from smallest to largest, with the smallest difference getting a rank of 1, then next larger difference getting a rank of 2, etc. ties are given average ranks. the ranks of all differences in one direction are summed, and the ranks of all differences in the other direction are summed. the smaller of these two sums is the test statistic, w (sometimes symbolized ts). unlike most test statistics, smaller values of w are less likely under the null hypothesis.
process;mauchly's test for sphericity;the mauchly's test for sphericity is a statistical test which evaluates if the variance of the differences between all combinations of the groups are equal, a property known as 'sphericity' in the context of repeated measures. it is used for instance prior to repeated measure anova. the test works by assessing if a wishart-distributed covariance matrix (or transformation thereof) is proportional to a given matrix.
process;high throughput screening;high throughput screening is a kind of investigation which uses a standardized assays (cell based, enzymatic or chemometric) to test the effect of substances (rnai or small molecules) held in libraries on a very specific and measureable outcome (e.g fluorence intensity). it relies on robotic handling to ensure fast and high-throughput in assay performance, data acquisition and hit selection.
process;roy s maximum root test;"roy's maximum root test finds the maximum characteristic root or eigenvalue statistic for testing equality of k p-variate normal distributions with same covariance matrix, independence between two sets of variables jointly distributed as a normal distribution, equality of covariance matrices of two p-variate normal distributions, whether the covariance matrix of a p-variabte normal distribution with unknown mean vector equals a specified matrix"
process;regression data imputation;regression data imputation is a type of data imputation where missing values are replaced with the value of a regression function coefficient.
process;between group comparison statistical test;between group comparison statistical test is a statistical test which aims to detect difference between the means computing for each of the study group populations
process;multivariate imputation with chained equations;multivariate imputation with chained equations (mice) is a type of data imputation which uses an algorithm devised by stef van buuren and karin groothuis-oudshoorn
process;levene's test;levene's test is a null hypothesis statistical test which evaluates the null hypothesis of equality of variance in several populations.
process;hit selection;hit selection is a planned process which in screening processes such as high-throughput screening, lead to the identification of perturbing agent which cause the typical signal generated by a standardized assay to significantly differ from the negative control. the selection hitself results from meeting or exceeding selection threshold (for instance 6 sigma from the mean or ssmd value beyond 5 when compared to positive controls or below -5 when compared to negative controls
process;between-within denominator degrees of freedom approximation;a data transformation to determine the number of degree of freedom
process;genomic best linear unbiased prediction;a data transformation which calculate predictions of breeding values using an animal model and a relationship matrix calculated from the genomic/genetic markers (g matrix), in constrast to using pedigree information as in blup, also known as ablup
process;least significance different test;the lsd test is a statistical test for multiple comparisons of treatments by means of least significant difference following an anova analysis
process;ordered probit regression for analysis of ordinal dependent variable;probit regression model is a model which attempts to explain data distribution associated with *ordinal* response/dependent variable in terms of values assumed by the independent variable uses a function of predictor/independent variable(s): the function used in this instance of regression modeling is the ordered probit function.
process;barlett's test;bartlett's test (see snedecor and cochran, 1989) is used to test if k samples are from populations with equal variances. equal variances across samples is called homoscedasticity or homogeneity of variances. some statistical tests, for example the analysis of variance, assume that variances are equal across groups or samples. the bartlett test can be used to verify that assumption. bartlett's test is sensitive to departures from normality. that is, if the samples come from non-normal distributions, then bartlett's test may simply be testing for non-normality. levene's test and the brown forsythe test are alternatives to the bartlett test that are less sensitive to departures from normality.
process;substitution by the mean data imputation;substitution by the mean data imputation is a type of data imputation where missing values are replaced with the value the variable mean.
process;goodness of fit statistical test;a goodness of fit statistical test is a statistical test which aim to evaluate if a sample distribution can be considered equivalent to a theoretical distribution used as input
process;specifying null and alternate hypothesis;a planned process which etablishes and states the different hypothesis to be evaluated during a null hypothesis statistical test
process;bayesian least absolute shrinkage and selection operator;bayesian lasso is a data transformation where the regression parameters have independent laplace (i.e., double-exponential) priors and are used to interprete lasso estimate for linear regression parameters as bayesian posterior mode estimates in accordance to a bayesian framework.
process;high-content screening;high content screening is a kind of investigation which uses a standardized cellular assays to test the effect of substances (rnai or small molecules) held in libraries on a cellular phenotype. it relies on microscopy imaging and or flow-cytometry, robotic handling to ensure fast and high-throughput.
process;voluntary sampling;the voluntary sampling method is a type of non-probability sampling. a voluntary sample is made up of people who self-select into the survey. often, these subjects have a strong interest in the main topic of the survey. volunteers may be invited through advertisements on social media sites
process;post-hoc analysis;a post-hoc analysis is a statistical test carried out following an analysis of variance which ruled out the null hypothesis of absence of difference between group which allows identifying which groups differ.
process;two-way anova;two-way anova is an analysis of variance where the different groups being compared are associated the factor levels of exatly 2 independent variables. the null hypothesis is an absence of difference between the means calculated for each of the groups. the test assumes normality and equivariance of the data.
process;rao's score test;the rao-scott test is a statistical test which tests the hypothesis that all coefficients associated with a particular regression term are zero (or have some other specified values). the lrt uses a linear combination of chisquared distributions
process;haybittle-peto boundary analysis;the haybittle peto boundary analysis is an interim analysis where a rule for deciding when to stop a clinical trial prematurely is defined. it is named for john haybittle and richard peto. the haybittle peto boundary is one such stopping rule, and it states that if an interim analysis shows a probability of equal to, or less than 0.001 that a difference as extreme or more between the treatments is found, given that the null hypothesis is true, then the trial should be stopped early. the final analysis is still evaluated at the normal level of significance (usually 0.05).[3][4] the main advantage of the haybittle peto boundary is that the same threshold is used at every interim analysis, unlike the o'brien fleming boundary, which changes at every analysis. also, using the haybittle peto boundary means that the final analysis is performed using a 0.05 level of significance as normal, which makes it easier for investigators and readers to understand. the main argument against the haybittle peto boundary is that some investigators believe that the haybittle peto boundary is too conservative and makes it too difficult to stop a trial. as all frequentist methods of the same type, it focuses on controlling the type i error rate as the repeated hypothesis testing of accumulating data increases the type i error rate of a clinical trial.
process;quadrat sampling;quadrat sampling is a classic tool for the study of ecology, especially biodiversity. in general, a series of squares (quadrats) of a set size are placed in a habitat of interest and the species within those quadrats are identified and recorded. passive quadrat sampling (done without removing the organisms found within the quadrat) can be either done by hand, with researchers carefully sorting through each individual quadrat or, more efficiently, can be done by taking a photograph of the quadrat for future analysis.
process;last observation carried forward data imputation;last observation carried forward data imputation is a type of data imputation which uses a very simple, self explanatory method for substituted a missing value for an observation. it should be noted that this method gives a biased estimate of the treatment effect and underestimates the variability of the estimated result and should be used cautiously.
process;statistical inference;statistical inference is the process of deducing properties of an underlying probability distribution by analysis of data.
process;generalized least squares estimation;the generalized least squares estimation is a model parameter estimation for a linear regression model with errors that are dependent and (possibly) have heterogeneous variance. difficult to use use in practice, as covariance matrix of the errors must known to "whiten" data and model. if true covariance is known, it is the best linear unbiased estimation (blue) method under these assumptions, uniformly minimum-variance unbiased estimator (umvue) with addition of a gaussian assumption.
process;breeding value estimation using genotype data;breeding value estimation is a data transformation process aiming at computing breeding value estimates of an organism given a set of genomic (snp) observations.
process;probability-proportional-to-size sampling;probability proportional to size ('pps') sampling is a sampling method in which the selection probability for each element is set to be proportional to its size measure, up to a maximum of 1. in a simple pps design, these selection probabilities can then be used as the basis for poisson sampling. however, this has the drawback of variable sample size, and different portions of the population may still be over- or under-represented due to chance variation in selections.
process;degree of freedom calculation;degree of freedom calculation is a data transformation which is part of a stastical test and which aims to determine or estimate the number of degrees of freedom in a system.
process;brown forsythe test;the brown forsythe test is a statistical test which evaluates if the variance of different groups are equal. it relies on computing the median rather than the mean, as used in the levene's test for homoschedacity. this test maybe used to, for instance, ensure that the conditions of applications of anova are met.
process;multinomial probit regression for analysis of polychotomous dependent variable;multinomial logistic regression model is a model which attempts to explain data distribution associated with *polychotomous* response/dependent variable in terms of values assumed by the independent variable uses a function of predictor/independent variable(s): the function used in this instance of regression modeling is probit function.
process;singular value decomposition;a data transformation which compute the singular-value decomposition of a rectangular matrix. the singular-value decomposition is very general in the sense that it can be applied to any m n matrix whereas eigenvalue decomposition can only be applied to certain classes of square matrices.
process;population stratification prior to sampling;stratification is a planned process which executes a stratification rule using as input a population and assign it member to mutually exclusive subpopulation based on the values defined by the stratification rule
process;anderson-darling test;the anderson darling test is a statistical test of whether a given sample of data is drawn from a given probability distribution.
process;z-test;z-test is a statistical test which evaluate the null hypothesis that the means of 2 populations are equal and returns a p-value.
process;shapiro-wilk test;shapiro-wilk test is a goodness of fit test which evaluates the null hypothesis that the sample is drawn from a population following a normal distribution
process;breeding value estimation;breeding value estimation is a data transformation process aiming at computing breeding value estimates of an organism given a set of genomic (snp) observations, pedigree information and/or phenotypic observations.
process;kolmogorov-smirnov test;kolmogorov-smirnov test is a goodness of fit test which evaluates the null hypothesis that a sample is drawn from a population that follows a specific continuous probability distribution.
process;statistical sampling;statistical sampling is a planned process which aims at assembling a population of observation units (samples) in as an unbiaised manner as possible in order to obtain or infer information about the actual population these samples have been drawn.
process;confidence interval calculation;confidence interval calculation is a data transformation which determines a confidence interval for a given statistical parameter
process;one sample t-test;one sample t-test is a kind of student's t-test which evaluates if a given sample can be reasonably assumed to be taken from the population. the test compares the sample statistic (m) to the population parameter (m). the one sample t-test is the small sample analog of the z test, which is suitable for large samples.
process;k-nearest neighbour data imputation;k-nearest neighbour imputation is a data imputation which uses the k-nearest neighbour algorithm to compute a substitution value for the missing values. for every observation to be imputed, it identifies k closest observations based on the euclidean distance and computes the weighted average (weighted based on distance) of these k obs.
process;odds ratio homogeneity test;odds ratio homogeneity test is a statistical test which aims to evaluate that null the hypothesis of consistency odds ratio accross different strata of population is true or not
process;cochran-armitage test for trend;the cochran-armitage test is a statistical test used in categorical data analysis when the aim is to assess for the presence of an association between a dichotomous variable (variable with two categories) and a polychotomous variable (a variable with k categories). the two-level variable represents the response, and the other represents an explanatory variable with ordered levels. the null hypothesis is the hypothesis of no trend, which means that the binomial proportion is the same for all levels of the explanatory variable for example, doses of a treatment can be ordered as 'low', 'medium', and 'high', and we may suspect that the treatment benefit cannot become smaller as the dose increases. the trend test is often used as a genotype-based test for case-control genetic association studies. 
process;model fitting;model fitting is a data transformation process which evaluates if a model appropriately represents a dataset. a model fitting process tests the goodness of fit of the model to the data
process;breeding value estimation using phenotypic data;breeding value estimation is a data transformation process aiming at computing breeding value estimates of an organism given a set of phenotypic observations.
process;stratified sampling;stratified sampling is a statistical sampling method which divides the population into homogenous subpopulations, which are then sampled using random or systematic sampling methods
process;group assignment based on blocking variable specification;group assignment based on blocking variable specification is a kind of group assignment process which takes into account the levels assumed by a blocking variable to allocate subjects or experimental units to a treatment group
process;hypergeometric test;hypergeometric test is a null hypothesis test which evaluates if a random variable follows a hypergeometric distribution. it is a test of goodness of fit to that distribution. the test is suited for situation aimed at assessing cases of sampling from a finite set without replacements. for instance, testing for enrichment or depletion of elements (e.g go categories, genes)
process;multivariate analysis of variance;"the multivariate analysis of variance, or manova, is a procedure for comparing multivariate sample means. as a multivariate procedure, it is used when there are two or more dependent variables, and is typically followed by significance tests involving individual dependent variables separately. it helps to answer: 1. do changes in the independent variable(s) have significant effects on the dependent variables? 2. what are the relationships among the dependent variables? 3. what are the relationships among the independent variables?"
process;breslow-day test for homogeneity of odds ratio;the breslow-day test is a statistical test which evaluates if the odds ratios are homogenous across n 2x2 contingency tables, for instance several 2x2 contingency tables associated with different strata of a stratified population when evaluating the relationship between exposure and outcome or associated with the different samples coming from several centres in a multicentric study in clinical trial context.
process;model parameter estimation;model parameter estimation is a data transformation that finds parameter values (the model parameter estimates) most compatible with the data as judged by the model.
process;multiway anova;multi-way anova is an analysis of variance where the difference groups being compared are associated to the factor levels of more than 2 independent variables. the null hypothesis is an absence of difference between the means calculated for each of the groups. the test assumes normality and equivariance of the data.
process;binary classification;binary classification (or binomial classification) is a data transformation which aims to cast members of a set into 2 disjoint groups depending on whether the element have a given property/feature or not.
process;one tailed test;a one-tailed test is a statistical test which, assuming an unskewed probability distribution, allocates all of the significance level to evaluate only one hypothesis to explain a difference. the one-tailed test provides more power to detect an effect in one direction by not testing the effect in the other direction. one-tailed test should be preceded by two-tailed test in order to avoid missing out on detecting alternate effect explaining an observed difference.
process;ordinary least squares estimation;the ordinary least squares estimation is a model parameter estimation for a linear regression model when the errors are uncorrelated and equal in variance. is the best linear unbiased estimation (blue) method under these assumptions, uniformly minimum-variance unbiased estimator (umvue) with addition of a gaussian assumption.
process;contrast estimation;a data transformation that finds a contrast value (the contrast estimate) by computing the weighted sum of model parameter estimates using a set of contrast weights.
process;multinomial logistic regression for analysis of dichotomous dependent variable;multinomial logistic regression model is a model which attempts to explain data distribution associated with *polychotomous* response/dependent variable in terms of values assumed by the independent variable uses a function of predictor/independent variable(s): the function used in this instance of regression modeling is logistic function.
process;minimax sampling;in imbalanced datasets, where the sampling ratio does not follow the population statistics, one can resample the dataset in a conservative manner called minimax sampling. the minimax sampling has its origin in anderson minimax ratio whose value is proved to be 0.5: in a binary classification, the class-sample sizes should be chosen equally. this ratio can be proved to be minimax ratio only under the assumption of lda classifier with gaussian distributions. the notion of minimax sampling is recently developed for a general class of classification rules, called class-wise smart classifiers.
process;two sample hotelling t2 test;hotelling's t2 test is a statistical test which is a generalization of student's t-test to a assess if the means of a set of variables remains unchanged when studying 2 populations. it is a type of multivariate analysis
process;bayesian model selection;a bayesian model selection is a data transformation which is based on bayesian statistics to compute bayes factor in order to evaluate which model best explains data.
process;wilk's lambda test;"wilks' lambda is a test statistic used in multivariate analysis of variance (manova) to test whether there are differences between the means of identified groups of subjects on a combination of dependent variables."
process;kenward-roger degree of freedom approximation;the kenward-roger method's fundamental idea is to calculate the approximate mean and variance of their statistic and then match moments with an f distribution to obtain the denominator degrees of freedom.
process;kruskal wallis test;the kruskal wallis test is a null hypothesis statistical testing objective which allows multiple (n>=2) groups (or conditions or treatments) to be compared, without making the assumption that values are normally distributed. the kruskal wallis test is the non-parametric equivalent of the independent samples anova. the kruskal wallis test is most commonly used when there is one nominal variable and one measurement variable, and the measurement variable does not meet the normality assumption of an anova.
process;one sample hotelling t2 test;the one-sample hotelling s t2 is the multivariate extension of the common one-sample or paired student s t-test. in a one-sample t-test, the mean response is compared against a specific value. hotelling s one-sample t2 is used when the number of response variables is two or more, although it can be used when there is only one response variable. t2 makes the usual assumption that the data are approximately multivariate normal. randomization tests are provided that do not rely on this assumption. these randomization tests should be used whenever you want exact results that do not rely on several assumptions.
process;tukey hsd for post-hoc analysis;tukey honestly significant difference (hsd) test is a statistical test used following an anova test yielding a statistically significant p-value in order to determine which means are different, to a given level of significance. the tukey hsd test relies on the q-distribution. the procedure is conservative, meaning that if sample sizes (the sizes of different study groups) are equal, the risk of a type i error is exactly , and if sample sizes are unequal it s less than .
process;mann-whitney u-test;the mann-whitney u-test is a null hypothesis statistical testing procedure which allows two groups (or conditions or treatments) to be compared without making the assumption that values are normally distributed. the mann-whitney test is the non-parametric equivalent of the t-test for independent samples
process;quota sampling;quota sampling is a method for selecting survey participants that is a non-probabilistic version of stratified sampling. in quota sampling, a population is first segmented into mutually exclusive sub-groups, just as in stratified sampling. then judgment is used to select the subjects or units from each segment based on a specified proportion. for example, an interviewer may be told to sample 200 females and 300 males between the age of 45 and 60. this means that individuals can put a demand on who they want to sample (targeting).
